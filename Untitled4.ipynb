{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deffdb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import logging\n",
    "\n",
    "# import Pysyft to help us to simulate federated leraning\n",
    "import syft as sy\n",
    "\n",
    "# hook PyTorch to PySyft i.e. add extra functionalities to support Federated Learning\n",
    "# and other private AI tools\n",
    "hook = sy.TorchHook(torch) \n",
    "\n",
    "# we create two imaginary schools\n",
    "westside_school = sy.VirtualWorker(hook, id=\"westside\")\n",
    "grapevine_high = sy.VirtualWorker(hook, id=\"grapevine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0fab7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the args\n",
    "args = {\n",
    "    'use_cuda' : True,\n",
    "    'batch_size' : 64,\n",
    "    'test_batch_size' : 1000,\n",
    "    'lr' : 0.01,\n",
    "    'log_interval' : 10,\n",
    "    'epochs' : 10\n",
    "}\n",
    "\n",
    "# check to use GPU or not\n",
    "use_cuda = args['use_cuda'] and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a79883c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple CNN net\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, stride = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32,out_channels = 64, kernel_size = 3, stride = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=64*12*12, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=10),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout2d(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = x.view(-1, 64*12*12)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c4a277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we take the help of PySyft's awesome API to prepare the data for us and\n",
    "# distribute for us across 2 workers ie. two schools\n",
    "# normally we dont have to distribute data, data is already there at the site.\n",
    "# We are doing this just to simulate federated learning.\n",
    "# Below code looks just like torch code with just some minor changes. This is what's nice about PySyft.\n",
    "federated_train_loader = sy.FederatedDataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "    .federate((grapevine_high, westside_school)),\n",
    "    batch_size=args['batch_size'], shuffle=True)\n",
    "\n",
    "# test data remains with us locally\n",
    "# this is the normal torch code to load test data from MNIST\n",
    "# that we are all familiar with\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args['test_batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a7d1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    # iterate over federated data\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        # send the model to the remote location \n",
    "        model = model.send(data.location)\n",
    "\n",
    "        # the same torch code that we are use to\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        # this loss is a ptr to the tensor loss \n",
    "        # at the remote location\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # call backward() on the loss ptr,\n",
    "        # that will send the command to call\n",
    "        # backward on the actual loss tensor\n",
    "        # present on the remote machine\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # get back the updated model\n",
    "        model.get()\n",
    "\n",
    "        if batch_idx % args['log_interval'] == 0:\n",
    "\n",
    "            # a thing to note is the variable loss was\n",
    "            # also created at remote worker, so we need to\n",
    "            # explicitly get it back\n",
    "            loss = loss.get()\n",
    "\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, \n",
    "                    batch_idx * args['batch_size'], # no of images done\n",
    "                    len(train_loader) * args['batch_size'], # total images left\n",
    "                    100. * batch_idx / len(train_loader), \n",
    "                    loss.item()\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87b3eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            # add losses together\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() \n",
    "\n",
    "            # get the index of the max probability class\n",
    "            pred = output.argmax(dim=1, keepdim=True)  \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cafe4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.321569\n",
      "Train Epoch: 1 [640/60032 (1%)]\tLoss: 2.208241\n",
      "Train Epoch: 1 [1280/60032 (2%)]\tLoss: 2.030099\n",
      "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 1.720557\n",
      "Train Epoch: 1 [2560/60032 (4%)]\tLoss: 1.266374\n",
      "Train Epoch: 1 [3200/60032 (5%)]\tLoss: 0.967603\n",
      "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 0.981259\n",
      "Train Epoch: 1 [4480/60032 (7%)]\tLoss: 0.759046\n",
      "Train Epoch: 1 [5120/60032 (9%)]\tLoss: 0.578204\n",
      "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 0.734796\n",
      "Train Epoch: 1 [6400/60032 (11%)]\tLoss: 0.509734\n",
      "Train Epoch: 1 [7040/60032 (12%)]\tLoss: 0.498562\n",
      "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 0.358980\n",
      "Train Epoch: 1 [8320/60032 (14%)]\tLoss: 0.304644\n",
      "Train Epoch: 1 [8960/60032 (15%)]\tLoss: 0.486878\n",
      "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.481473\n",
      "Train Epoch: 1 [10240/60032 (17%)]\tLoss: 0.257102\n",
      "Train Epoch: 1 [10880/60032 (18%)]\tLoss: 0.604763\n",
      "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.244456\n",
      "Train Epoch: 1 [12160/60032 (20%)]\tLoss: 0.309273\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args['lr'])\n",
    "\n",
    "logging.info(\"Starting training !!\")\n",
    "\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "        train(args, model, device, federated_train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "    \n",
    "# thats all we need to do XD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c8e367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a21c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
