{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d5408c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import logging\n",
    "from pandas import read_csv\n",
    "import syft as sy\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ca2187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid ,ReLU\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch import Tensor\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 32)\n",
    "        xavier_uniform_(self.hidden1.weight)\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(32, 16)\n",
    "        xavier_uniform_(self.hidden2.weight)\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(16, 5)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75b0ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/qoe/federated_devices/pokemon_complet.csv'\n",
    "df = read_csv(path)\n",
    "\n",
    "train_set=df.sample(frac=0.8,random_state=200) #random state is a seed value\n",
    "test_set=df.drop(train_set.index)\n",
    "# test data\n",
    "test_dl = DataLoader(test_set, batch_size=32, shuffle=True)\n",
    "\n",
    "# slipt the data into 3 devices dataset\n",
    "train_set1 =  train_set.loc[train_set['QoD_model'].isin([5, 8, 9])]\n",
    "train_set2 =  train_set.loc[train_set['QoD_model'].isin([2, 1, 9])]\n",
    "train_set3 =  train_set.loc[train_set['QoD_model'].isin([3, 4, 6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef7034bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device1 dataset\n",
    "train_set1.rename(columns=train_set1.iloc[0]).drop(train_set1.index[0])\n",
    "train_set_data1  = train_set1.iloc[:, 0:16].astype('float32')\n",
    "target_set_data1 = train_set1.iloc[:, 16:21].astype('float32')\n",
    "\n",
    "\n",
    "train_tensor1 = torch.from_numpy(train_set_data1.to_numpy()).float()\n",
    "target_tensor1  = torch.from_numpy(target_set_data1.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12064214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device2 dataset\n",
    "train_set2.rename(columns=train_set2.iloc[0]).drop(train_set2.index[0])\n",
    "train_set_data2  = train_set2.iloc[:, 0:16].astype('float32')\n",
    "target_set_data2 = train_set2.iloc[:, 16:21].astype('float32')\n",
    "\n",
    "\n",
    "train_tensor2 = torch.from_numpy(train_set_data2.to_numpy()).float()\n",
    "target_tensor2  = torch.from_numpy(target_set_data2.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bfee9f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device3 dataset\n",
    "train_set3.rename(columns=train_set3.iloc[0]).drop(train_set3.index[0])\n",
    "train_set_data3  = train_set3.iloc[:, 0:16].astype('float32')\n",
    "target_set_data3 = train_set3.iloc[:, 16:21].astype('float32')\n",
    "\n",
    "\n",
    "train_tensor3 = torch.from_numpy(train_set_data3.to_numpy()).float()\n",
    "target_tensor3  = torch.from_numpy(target_set_data3.to_numpy()).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74806cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bef79dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "# Create the federated enviroment \n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "joe = sy.VirtualWorker(hook, id=\"joe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a6106f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the federated dataset \n",
    "target_tensor_1 = target_tensor1.type(torch.LongTensor)\n",
    "target_tensor_2 = target_tensor2.type(torch.LongTensor)\n",
    "target_tensor_3 = target_tensor3.type(torch.LongTensor)\n",
    "\n",
    "# sending the data set to the devices\n",
    "bob_train_dataset = sy.BaseDataset(train_tensor1,target_tensor1).send(bob) \n",
    "anne_train_dataset = sy.BaseDataset(train_tensor2, target_tensor2).send(alice)\n",
    "joe_train_dataset = sy.BaseDataset(train_tensor3, target_tensor3).send(joe)\n",
    "\n",
    "federated_train_dataset = sy.FederatedDataset([bob_train_dataset, anne_train_dataset,joe_train_dataset]) \n",
    "federated_train_loader = sy.FederatedDataLoader(federated_train_dataset, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88c4f063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FederatedDataset\n",
      "    Distributed accross: bob, alice, joe\n",
      "    Number of datapoints: 1245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(federated_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "23f56b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = MLP(16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8f380a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<syft.frameworks.torch.fl.dataloader.FederatedDataLoader object at 0x7ffc29ee1ee0>\n"
     ]
    }
   ],
   "source": [
    "print(federated_train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9239ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, federate_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    #criterion = MSELoss()\n",
    "    criterion = L1Loss()\n",
    "    for batch_idx, (data, target) in enumerate(federate_train_loader):\n",
    "        model.send(data.location)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #loss = F.nll_loss(output, target.float())\n",
    "        loss = criterion(output, target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get()\n",
    "        if batch_idx % 10 == 0:\n",
    "            loss = loss.get()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, \n",
    "                    batch_idx * 32 , # no of images done\n",
    "                    len(federate_train_loader) * 32, # total images left\n",
    "                    100. * batch_idx / len(federate_train_loader), \n",
    "                    loss.item()\n",
    "                 )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20c6d04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1248 (0%)]\tLoss: 1028.902954\n",
      "Train Epoch: 1 [320/1248 (26%)]\tLoss: 4.014359\n",
      "Train Epoch: 1 [640/1248 (51%)]\tLoss: 3.577719\n",
      "Train Epoch: 1 [960/1248 (77%)]\tLoss: 3.680396\n",
      "Train Epoch: 2 [0/1248 (0%)]\tLoss: 3.678448\n",
      "Train Epoch: 2 [320/1248 (26%)]\tLoss: 3.659419\n",
      "Train Epoch: 2 [640/1248 (51%)]\tLoss: 3.249482\n",
      "Train Epoch: 2 [960/1248 (77%)]\tLoss: 3.331533\n",
      "Train Epoch: 3 [0/1248 (0%)]\tLoss: 3.347487\n",
      "Train Epoch: 3 [320/1248 (26%)]\tLoss: 3.289799\n",
      "Train Epoch: 3 [640/1248 (51%)]\tLoss: 2.921957\n",
      "Train Epoch: 3 [960/1248 (77%)]\tLoss: 2.996234\n",
      "Train Epoch: 4 [0/1248 (0%)]\tLoss: 3.013695\n",
      "Train Epoch: 4 [320/1248 (26%)]\tLoss: 2.927431\n",
      "Train Epoch: 4 [640/1248 (51%)]\tLoss: 2.634166\n",
      "Train Epoch: 4 [960/1248 (77%)]\tLoss: 2.709378\n",
      "Train Epoch: 5 [0/1248 (0%)]\tLoss: 2.712207\n",
      "Train Epoch: 5 [320/1248 (26%)]\tLoss: 2.587116\n",
      "Train Epoch: 5 [640/1248 (51%)]\tLoss: 2.350523\n",
      "Train Epoch: 5 [960/1248 (77%)]\tLoss: 2.425320\n",
      "Train Epoch: 6 [0/1248 (0%)]\tLoss: 2.386070\n",
      "Train Epoch: 6 [320/1248 (26%)]\tLoss: 2.264428\n",
      "Train Epoch: 6 [640/1248 (51%)]\tLoss: 2.062916\n",
      "Train Epoch: 6 [960/1248 (77%)]\tLoss: 2.159380\n",
      "Train Epoch: 7 [0/1248 (0%)]\tLoss: 2.105803\n",
      "Train Epoch: 7 [320/1248 (26%)]\tLoss: 1.973983\n",
      "Train Epoch: 7 [640/1248 (51%)]\tLoss: 1.806283\n",
      "Train Epoch: 7 [960/1248 (77%)]\tLoss: 1.908953\n",
      "Train Epoch: 8 [0/1248 (0%)]\tLoss: 1.868239\n",
      "Train Epoch: 8 [320/1248 (26%)]\tLoss: 1.727110\n",
      "Train Epoch: 8 [640/1248 (51%)]\tLoss: 1.558817\n",
      "Train Epoch: 8 [960/1248 (77%)]\tLoss: 1.677882\n",
      "Train Epoch: 9 [0/1248 (0%)]\tLoss: 1.642271\n",
      "Train Epoch: 9 [320/1248 (26%)]\tLoss: 1.482103\n",
      "Train Epoch: 9 [640/1248 (51%)]\tLoss: 1.328833\n",
      "Train Epoch: 9 [960/1248 (77%)]\tLoss: 1.441597\n",
      "Train Epoch: 10 [0/1248 (0%)]\tLoss: 1.416717\n",
      "Train Epoch: 10 [320/1248 (26%)]\tLoss: 1.279775\n",
      "Train Epoch: 10 [640/1248 (51%)]\tLoss: 1.163993\n",
      "Train Epoch: 10 [960/1248 (77%)]\tLoss: 1.270887\n",
      "Train Epoch: 11 [0/1248 (0%)]\tLoss: 1.240839\n",
      "Train Epoch: 11 [320/1248 (26%)]\tLoss: 1.100829\n",
      "Train Epoch: 11 [640/1248 (51%)]\tLoss: 1.004223\n",
      "Train Epoch: 11 [960/1248 (77%)]\tLoss: 1.110569\n",
      "Train Epoch: 12 [0/1248 (0%)]\tLoss: 1.067612\n",
      "Train Epoch: 12 [320/1248 (26%)]\tLoss: 0.930650\n",
      "Train Epoch: 12 [640/1248 (51%)]\tLoss: 0.868971\n",
      "Train Epoch: 12 [960/1248 (77%)]\tLoss: 0.967696\n",
      "Train Epoch: 13 [0/1248 (0%)]\tLoss: 0.940372\n",
      "Train Epoch: 13 [320/1248 (26%)]\tLoss: 0.795572\n",
      "Train Epoch: 13 [640/1248 (51%)]\tLoss: 0.777749\n",
      "Train Epoch: 13 [960/1248 (77%)]\tLoss: 0.876822\n",
      "Train Epoch: 14 [0/1248 (0%)]\tLoss: 0.844144\n",
      "Train Epoch: 14 [320/1248 (26%)]\tLoss: 0.710592\n",
      "Train Epoch: 14 [640/1248 (51%)]\tLoss: 0.723926\n",
      "Train Epoch: 14 [960/1248 (77%)]\tLoss: 0.812231\n",
      "Train Epoch: 15 [0/1248 (0%)]\tLoss: 0.743775\n",
      "Train Epoch: 15 [320/1248 (26%)]\tLoss: 0.613649\n",
      "Train Epoch: 15 [640/1248 (51%)]\tLoss: 0.692546\n",
      "Train Epoch: 15 [960/1248 (77%)]\tLoss: 0.740840\n",
      "Train Epoch: 16 [0/1248 (0%)]\tLoss: 0.686113\n",
      "Train Epoch: 16 [320/1248 (26%)]\tLoss: 0.585919\n",
      "Train Epoch: 16 [640/1248 (51%)]\tLoss: 0.666230\n",
      "Train Epoch: 16 [960/1248 (77%)]\tLoss: 0.741419\n",
      "Train Epoch: 17 [0/1248 (0%)]\tLoss: 0.646685\n",
      "Train Epoch: 17 [320/1248 (26%)]\tLoss: 0.565322\n",
      "Train Epoch: 17 [640/1248 (51%)]\tLoss: 0.685780\n",
      "Train Epoch: 17 [960/1248 (77%)]\tLoss: 0.740975\n",
      "Train Epoch: 18 [0/1248 (0%)]\tLoss: 0.626514\n",
      "Train Epoch: 18 [320/1248 (26%)]\tLoss: 0.540583\n",
      "Train Epoch: 18 [640/1248 (51%)]\tLoss: 0.658650\n",
      "Train Epoch: 18 [960/1248 (77%)]\tLoss: 0.720411\n",
      "Train Epoch: 19 [0/1248 (0%)]\tLoss: 0.609659\n",
      "Train Epoch: 19 [320/1248 (26%)]\tLoss: 0.517632\n",
      "Train Epoch: 19 [640/1248 (51%)]\tLoss: 0.664367\n",
      "Train Epoch: 19 [960/1248 (77%)]\tLoss: 0.732182\n",
      "Train Epoch: 20 [0/1248 (0%)]\tLoss: 0.640115\n",
      "Train Epoch: 20 [320/1248 (26%)]\tLoss: 0.489363\n",
      "Train Epoch: 20 [640/1248 (51%)]\tLoss: 0.665381\n",
      "Train Epoch: 20 [960/1248 (77%)]\tLoss: 0.722470\n",
      "Train Epoch: 21 [0/1248 (0%)]\tLoss: 0.591691\n",
      "Train Epoch: 21 [320/1248 (26%)]\tLoss: 0.499584\n",
      "Train Epoch: 21 [640/1248 (51%)]\tLoss: 0.664210\n",
      "Train Epoch: 21 [960/1248 (77%)]\tLoss: 0.694952\n",
      "Train Epoch: 22 [0/1248 (0%)]\tLoss: 0.570957\n",
      "Train Epoch: 22 [320/1248 (26%)]\tLoss: 0.477429\n",
      "Train Epoch: 22 [640/1248 (51%)]\tLoss: 0.669523\n",
      "Train Epoch: 22 [960/1248 (77%)]\tLoss: 0.699965\n",
      "Train Epoch: 23 [0/1248 (0%)]\tLoss: 0.583650\n",
      "Train Epoch: 23 [320/1248 (26%)]\tLoss: 0.485036\n",
      "Train Epoch: 23 [640/1248 (51%)]\tLoss: 0.664355\n",
      "Train Epoch: 23 [960/1248 (77%)]\tLoss: 0.710616\n",
      "Train Epoch: 24 [0/1248 (0%)]\tLoss: 0.546836\n",
      "Train Epoch: 24 [320/1248 (26%)]\tLoss: 0.472411\n",
      "Train Epoch: 24 [640/1248 (51%)]\tLoss: 0.677551\n",
      "Train Epoch: 24 [960/1248 (77%)]\tLoss: 0.690182\n",
      "Train Epoch: 25 [0/1248 (0%)]\tLoss: 0.589912\n",
      "Train Epoch: 25 [320/1248 (26%)]\tLoss: 0.451429\n",
      "Train Epoch: 25 [640/1248 (51%)]\tLoss: 0.666181\n",
      "Train Epoch: 25 [960/1248 (77%)]\tLoss: 0.710389\n",
      "Train Epoch: 26 [0/1248 (0%)]\tLoss: 0.544806\n",
      "Train Epoch: 26 [320/1248 (26%)]\tLoss: 0.449169\n",
      "Train Epoch: 26 [640/1248 (51%)]\tLoss: 0.671109\n",
      "Train Epoch: 26 [960/1248 (77%)]\tLoss: 0.698752\n",
      "Train Epoch: 27 [0/1248 (0%)]\tLoss: 0.547316\n",
      "Train Epoch: 27 [320/1248 (26%)]\tLoss: 0.487335\n",
      "Train Epoch: 27 [640/1248 (51%)]\tLoss: 0.666244\n",
      "Train Epoch: 27 [960/1248 (77%)]\tLoss: 0.698570\n",
      "Train Epoch: 28 [0/1248 (0%)]\tLoss: 0.576131\n",
      "Train Epoch: 28 [320/1248 (26%)]\tLoss: 0.470745\n",
      "Train Epoch: 28 [640/1248 (51%)]\tLoss: 0.665183\n",
      "Train Epoch: 28 [960/1248 (77%)]\tLoss: 0.689642\n",
      "Train Epoch: 29 [0/1248 (0%)]\tLoss: 0.547550\n",
      "Train Epoch: 29 [320/1248 (26%)]\tLoss: 0.456486\n",
      "Train Epoch: 29 [640/1248 (51%)]\tLoss: 0.669773\n",
      "Train Epoch: 29 [960/1248 (77%)]\tLoss: 0.690243\n",
      "Train Epoch: 30 [0/1248 (0%)]\tLoss: 0.562052\n",
      "Train Epoch: 30 [320/1248 (26%)]\tLoss: 0.452080\n",
      "Train Epoch: 30 [640/1248 (51%)]\tLoss: 0.672282\n",
      "Train Epoch: 30 [960/1248 (77%)]\tLoss: 0.690071\n",
      "Train Epoch: 31 [0/1248 (0%)]\tLoss: 0.552672\n",
      "Train Epoch: 31 [320/1248 (26%)]\tLoss: 0.477667\n",
      "Train Epoch: 31 [640/1248 (51%)]\tLoss: 0.669512\n",
      "Train Epoch: 31 [960/1248 (77%)]\tLoss: 0.690918\n",
      "Train Epoch: 32 [0/1248 (0%)]\tLoss: 0.591351\n",
      "Train Epoch: 32 [320/1248 (26%)]\tLoss: 0.450617\n",
      "Train Epoch: 32 [640/1248 (51%)]\tLoss: 0.677197\n",
      "Train Epoch: 32 [960/1248 (77%)]\tLoss: 0.708095\n",
      "Train Epoch: 33 [0/1248 (0%)]\tLoss: 0.565978\n",
      "Train Epoch: 33 [320/1248 (26%)]\tLoss: 0.453996\n",
      "Train Epoch: 33 [640/1248 (51%)]\tLoss: 0.670067\n",
      "Train Epoch: 33 [960/1248 (77%)]\tLoss: 0.691913\n",
      "Train Epoch: 34 [0/1248 (0%)]\tLoss: 0.562046\n",
      "Train Epoch: 34 [320/1248 (26%)]\tLoss: 0.461751\n",
      "Train Epoch: 34 [640/1248 (51%)]\tLoss: 0.670408\n",
      "Train Epoch: 34 [960/1248 (77%)]\tLoss: 0.699280\n",
      "Train Epoch: 35 [0/1248 (0%)]\tLoss: 0.567734\n",
      "Train Epoch: 35 [320/1248 (26%)]\tLoss: 0.457481\n",
      "Train Epoch: 35 [640/1248 (51%)]\tLoss: 0.673237\n",
      "Train Epoch: 35 [960/1248 (77%)]\tLoss: 0.709764\n",
      "Train Epoch: 36 [0/1248 (0%)]\tLoss: 0.567611\n",
      "Train Epoch: 36 [320/1248 (26%)]\tLoss: 0.457427\n",
      "Train Epoch: 36 [640/1248 (51%)]\tLoss: 0.673074\n",
      "Train Epoch: 36 [960/1248 (77%)]\tLoss: 0.699418\n",
      "Train Epoch: 37 [0/1248 (0%)]\tLoss: 0.591035\n",
      "Train Epoch: 37 [320/1248 (26%)]\tLoss: 0.468605\n",
      "Train Epoch: 37 [640/1248 (51%)]\tLoss: 0.665044\n",
      "Train Epoch: 37 [960/1248 (77%)]\tLoss: 0.702337\n",
      "Train Epoch: 38 [0/1248 (0%)]\tLoss: 0.589261\n",
      "Train Epoch: 38 [320/1248 (26%)]\tLoss: 0.454898\n",
      "Train Epoch: 38 [640/1248 (51%)]\tLoss: 0.676495\n",
      "Train Epoch: 38 [960/1248 (77%)]\tLoss: 0.711267\n",
      "Train Epoch: 39 [0/1248 (0%)]\tLoss: 0.546322\n",
      "Train Epoch: 39 [320/1248 (26%)]\tLoss: 0.466086\n",
      "Train Epoch: 39 [640/1248 (51%)]\tLoss: 0.711797\n",
      "Train Epoch: 39 [960/1248 (77%)]\tLoss: 0.689935\n",
      "Train Epoch: 40 [0/1248 (0%)]\tLoss: 0.580417\n",
      "Train Epoch: 40 [320/1248 (26%)]\tLoss: 0.481407\n",
      "Train Epoch: 40 [640/1248 (51%)]\tLoss: 0.672338\n",
      "Train Epoch: 40 [960/1248 (77%)]\tLoss: 0.690630\n",
      "Train Epoch: 41 [0/1248 (0%)]\tLoss: 0.593994\n",
      "Train Epoch: 41 [320/1248 (26%)]\tLoss: 0.451734\n",
      "Train Epoch: 41 [640/1248 (51%)]\tLoss: 0.678762\n",
      "Train Epoch: 41 [960/1248 (77%)]\tLoss: 0.690202\n",
      "Train Epoch: 42 [0/1248 (0%)]\tLoss: 0.566283\n",
      "Train Epoch: 42 [320/1248 (26%)]\tLoss: 0.454737\n",
      "Train Epoch: 42 [640/1248 (51%)]\tLoss: 0.675755\n",
      "Train Epoch: 42 [960/1248 (77%)]\tLoss: 0.711938\n",
      "Train Epoch: 43 [0/1248 (0%)]\tLoss: 0.546348\n",
      "Train Epoch: 43 [320/1248 (26%)]\tLoss: 0.455977\n",
      "Train Epoch: 43 [640/1248 (51%)]\tLoss: 0.669628\n",
      "Train Epoch: 43 [960/1248 (77%)]\tLoss: 0.689968\n",
      "Train Epoch: 44 [0/1248 (0%)]\tLoss: 0.578755\n",
      "Train Epoch: 44 [320/1248 (26%)]\tLoss: 0.484103\n",
      "Train Epoch: 44 [640/1248 (51%)]\tLoss: 0.676298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 44 [960/1248 (77%)]\tLoss: 0.693609\n",
      "Train Epoch: 45 [0/1248 (0%)]\tLoss: 0.546606\n",
      "Train Epoch: 45 [320/1248 (26%)]\tLoss: 0.486331\n",
      "Train Epoch: 45 [640/1248 (51%)]\tLoss: 0.665001\n",
      "Train Epoch: 45 [960/1248 (77%)]\tLoss: 0.701114\n",
      "Train Epoch: 46 [0/1248 (0%)]\tLoss: 0.565528\n",
      "Train Epoch: 46 [320/1248 (26%)]\tLoss: 0.453712\n",
      "Train Epoch: 46 [640/1248 (51%)]\tLoss: 0.672858\n",
      "Train Epoch: 46 [960/1248 (77%)]\tLoss: 0.692507\n",
      "Train Epoch: 47 [0/1248 (0%)]\tLoss: 0.549692\n",
      "Train Epoch: 47 [320/1248 (26%)]\tLoss: 0.465513\n",
      "Train Epoch: 47 [640/1248 (51%)]\tLoss: 0.707565\n",
      "Train Epoch: 47 [960/1248 (77%)]\tLoss: 0.701713\n",
      "Train Epoch: 48 [0/1248 (0%)]\tLoss: 0.560592\n",
      "Train Epoch: 48 [320/1248 (26%)]\tLoss: 0.460263\n",
      "Train Epoch: 48 [640/1248 (51%)]\tLoss: 0.671401\n",
      "Train Epoch: 48 [960/1248 (77%)]\tLoss: 0.698917\n",
      "Train Epoch: 49 [0/1248 (0%)]\tLoss: 0.545771\n",
      "Train Epoch: 49 [320/1248 (26%)]\tLoss: 0.486406\n",
      "Train Epoch: 49 [640/1248 (51%)]\tLoss: 0.675514\n",
      "Train Epoch: 49 [960/1248 (77%)]\tLoss: 0.690027\n",
      "Train Epoch: 50 [0/1248 (0%)]\tLoss: 0.579681\n",
      "Train Epoch: 50 [320/1248 (26%)]\tLoss: 0.480812\n",
      "Train Epoch: 50 [640/1248 (51%)]\tLoss: 0.671906\n",
      "Train Epoch: 50 [960/1248 (77%)]\tLoss: 0.690731\n",
      "Train Epoch: 51 [0/1248 (0%)]\tLoss: 0.593131\n",
      "Train Epoch: 51 [320/1248 (26%)]\tLoss: 0.451758\n",
      "Train Epoch: 51 [640/1248 (51%)]\tLoss: 0.678263\n",
      "Train Epoch: 51 [960/1248 (77%)]\tLoss: 0.692539\n",
      "Train Epoch: 52 [0/1248 (0%)]\tLoss: 0.563931\n",
      "Train Epoch: 52 [320/1248 (26%)]\tLoss: 0.452562\n",
      "Train Epoch: 52 [640/1248 (51%)]\tLoss: 0.673085\n",
      "Train Epoch: 52 [960/1248 (77%)]\tLoss: 0.691098\n",
      "Train Epoch: 53 [0/1248 (0%)]\tLoss: 0.582519\n",
      "Train Epoch: 53 [320/1248 (26%)]\tLoss: 0.459722\n",
      "Train Epoch: 53 [640/1248 (51%)]\tLoss: 0.668765\n",
      "Train Epoch: 53 [960/1248 (77%)]\tLoss: 0.711193\n",
      "Train Epoch: 54 [0/1248 (0%)]\tLoss: 0.546534\n",
      "Train Epoch: 54 [320/1248 (26%)]\tLoss: 0.469220\n",
      "Train Epoch: 54 [640/1248 (51%)]\tLoss: 0.677697\n",
      "Train Epoch: 54 [960/1248 (77%)]\tLoss: 0.688860\n",
      "Train Epoch: 55 [0/1248 (0%)]\tLoss: 0.545004\n",
      "Train Epoch: 55 [320/1248 (26%)]\tLoss: 0.469309\n",
      "Train Epoch: 55 [640/1248 (51%)]\tLoss: 0.676836\n",
      "Train Epoch: 55 [960/1248 (77%)]\tLoss: 0.692172\n",
      "Train Epoch: 56 [0/1248 (0%)]\tLoss: 0.550177\n",
      "Train Epoch: 56 [320/1248 (26%)]\tLoss: 0.456834\n",
      "Train Epoch: 56 [640/1248 (51%)]\tLoss: 0.671828\n",
      "Train Epoch: 56 [960/1248 (77%)]\tLoss: 0.697956\n",
      "Train Epoch: 57 [0/1248 (0%)]\tLoss: 0.588401\n",
      "Train Epoch: 57 [320/1248 (26%)]\tLoss: 0.450799\n",
      "Train Epoch: 57 [640/1248 (51%)]\tLoss: 0.675521\n",
      "Train Epoch: 57 [960/1248 (77%)]\tLoss: 0.694309\n",
      "Train Epoch: 58 [0/1248 (0%)]\tLoss: 0.591769\n",
      "Train Epoch: 58 [320/1248 (26%)]\tLoss: 0.451458\n",
      "Train Epoch: 58 [640/1248 (51%)]\tLoss: 0.702935\n",
      "Train Epoch: 58 [960/1248 (77%)]\tLoss: 0.692906\n",
      "Train Epoch: 59 [0/1248 (0%)]\tLoss: 0.552222\n",
      "Train Epoch: 59 [320/1248 (26%)]\tLoss: 0.451221\n",
      "Train Epoch: 59 [640/1248 (51%)]\tLoss: 0.704387\n",
      "Train Epoch: 59 [960/1248 (77%)]\tLoss: 0.699994\n",
      "Train Epoch: 60 [0/1248 (0%)]\tLoss: 0.546285\n",
      "Train Epoch: 60 [320/1248 (26%)]\tLoss: 0.488138\n",
      "Train Epoch: 60 [640/1248 (51%)]\tLoss: 0.664781\n",
      "Train Epoch: 60 [960/1248 (77%)]\tLoss: 0.689351\n",
      "Train Epoch: 61 [0/1248 (0%)]\tLoss: 0.578395\n",
      "Train Epoch: 61 [320/1248 (26%)]\tLoss: 0.483459\n",
      "Train Epoch: 61 [640/1248 (51%)]\tLoss: 0.681452\n",
      "Train Epoch: 61 [960/1248 (77%)]\tLoss: 0.695330\n",
      "Train Epoch: 62 [0/1248 (0%)]\tLoss: 0.551121\n",
      "Train Epoch: 62 [320/1248 (26%)]\tLoss: 0.468208\n",
      "Train Epoch: 62 [640/1248 (51%)]\tLoss: 0.676561\n",
      "Train Epoch: 62 [960/1248 (77%)]\tLoss: 0.692116\n",
      "Train Epoch: 63 [0/1248 (0%)]\tLoss: 0.548719\n",
      "Train Epoch: 63 [320/1248 (26%)]\tLoss: 0.468582\n",
      "Train Epoch: 63 [640/1248 (51%)]\tLoss: 0.706858\n",
      "Train Epoch: 63 [960/1248 (77%)]\tLoss: 0.701877\n",
      "Train Epoch: 64 [0/1248 (0%)]\tLoss: 0.545651\n",
      "Train Epoch: 64 [320/1248 (26%)]\tLoss: 0.481608\n",
      "Train Epoch: 64 [640/1248 (51%)]\tLoss: 0.708765\n",
      "Train Epoch: 64 [960/1248 (77%)]\tLoss: 0.710283\n",
      "Train Epoch: 65 [0/1248 (0%)]\tLoss: 0.546117\n",
      "Train Epoch: 65 [320/1248 (26%)]\tLoss: 0.469564\n",
      "Train Epoch: 65 [640/1248 (51%)]\tLoss: 0.676916\n",
      "Train Epoch: 65 [960/1248 (77%)]\tLoss: 0.690603\n",
      "Train Epoch: 66 [0/1248 (0%)]\tLoss: 0.589992\n",
      "Train Epoch: 66 [320/1248 (26%)]\tLoss: 0.448372\n",
      "Train Epoch: 66 [640/1248 (51%)]\tLoss: 0.699167\n",
      "Train Epoch: 66 [960/1248 (77%)]\tLoss: 0.702425\n",
      "Train Epoch: 67 [0/1248 (0%)]\tLoss: 0.547338\n",
      "Train Epoch: 67 [320/1248 (26%)]\tLoss: 0.457035\n",
      "Train Epoch: 67 [640/1248 (51%)]\tLoss: 0.670232\n",
      "Train Epoch: 67 [960/1248 (77%)]\tLoss: 0.716714\n",
      "Train Epoch: 68 [0/1248 (0%)]\tLoss: 0.555308\n",
      "Train Epoch: 68 [320/1248 (26%)]\tLoss: 0.452358\n",
      "Train Epoch: 68 [640/1248 (51%)]\tLoss: 0.668928\n",
      "Train Epoch: 68 [960/1248 (77%)]\tLoss: 0.691623\n",
      "Train Epoch: 69 [0/1248 (0%)]\tLoss: 0.586980\n",
      "Train Epoch: 69 [320/1248 (26%)]\tLoss: 0.451354\n",
      "Train Epoch: 69 [640/1248 (51%)]\tLoss: 0.676430\n",
      "Train Epoch: 69 [960/1248 (77%)]\tLoss: 0.694272\n",
      "Train Epoch: 70 [0/1248 (0%)]\tLoss: 0.589481\n",
      "Train Epoch: 70 [320/1248 (26%)]\tLoss: 0.449266\n",
      "Train Epoch: 70 [640/1248 (51%)]\tLoss: 0.700533\n",
      "Train Epoch: 70 [960/1248 (77%)]\tLoss: 0.694703\n",
      "Train Epoch: 71 [0/1248 (0%)]\tLoss: 0.546879\n",
      "Train Epoch: 71 [320/1248 (26%)]\tLoss: 0.460351\n",
      "Train Epoch: 71 [640/1248 (51%)]\tLoss: 0.681986\n",
      "Train Epoch: 71 [960/1248 (77%)]\tLoss: 0.691715\n",
      "Train Epoch: 72 [0/1248 (0%)]\tLoss: 0.588527\n",
      "Train Epoch: 72 [320/1248 (26%)]\tLoss: 0.451184\n",
      "Train Epoch: 72 [640/1248 (51%)]\tLoss: 0.676035\n",
      "Train Epoch: 72 [960/1248 (77%)]\tLoss: 0.708656\n",
      "Train Epoch: 73 [0/1248 (0%)]\tLoss: 0.559187\n",
      "Train Epoch: 73 [320/1248 (26%)]\tLoss: 0.456790\n",
      "Train Epoch: 73 [640/1248 (51%)]\tLoss: 0.670378\n",
      "Train Epoch: 73 [960/1248 (77%)]\tLoss: 0.689683\n",
      "Train Epoch: 74 [0/1248 (0%)]\tLoss: 0.577605\n",
      "Train Epoch: 74 [320/1248 (26%)]\tLoss: 0.482723\n",
      "Train Epoch: 74 [640/1248 (51%)]\tLoss: 0.675674\n",
      "Train Epoch: 74 [960/1248 (77%)]\tLoss: 0.712962\n",
      "Train Epoch: 75 [0/1248 (0%)]\tLoss: 0.574820\n",
      "Train Epoch: 75 [320/1248 (26%)]\tLoss: 0.466584\n",
      "Train Epoch: 75 [640/1248 (51%)]\tLoss: 0.708363\n",
      "Train Epoch: 75 [960/1248 (77%)]\tLoss: 0.689910\n",
      "Train Epoch: 76 [0/1248 (0%)]\tLoss: 0.575537\n",
      "Train Epoch: 76 [320/1248 (26%)]\tLoss: 0.462983\n",
      "Train Epoch: 76 [640/1248 (51%)]\tLoss: 0.706889\n",
      "Train Epoch: 76 [960/1248 (77%)]\tLoss: 0.702578\n",
      "Train Epoch: 77 [0/1248 (0%)]\tLoss: 0.581721\n",
      "Train Epoch: 77 [320/1248 (26%)]\tLoss: 0.447857\n",
      "Train Epoch: 77 [640/1248 (51%)]\tLoss: 0.676885\n",
      "Train Epoch: 77 [960/1248 (77%)]\tLoss: 0.691991\n",
      "Train Epoch: 78 [0/1248 (0%)]\tLoss: 0.548625\n",
      "Train Epoch: 78 [320/1248 (26%)]\tLoss: 0.458315\n",
      "Train Epoch: 78 [640/1248 (51%)]\tLoss: 0.669766\n",
      "Train Epoch: 78 [960/1248 (77%)]\tLoss: 0.715072\n",
      "Train Epoch: 79 [0/1248 (0%)]\tLoss: 0.556371\n",
      "Train Epoch: 79 [320/1248 (26%)]\tLoss: 0.453661\n",
      "Train Epoch: 79 [640/1248 (51%)]\tLoss: 0.668379\n",
      "Train Epoch: 79 [960/1248 (77%)]\tLoss: 0.691002\n",
      "Train Epoch: 80 [0/1248 (0%)]\tLoss: 0.574389\n",
      "Train Epoch: 80 [320/1248 (26%)]\tLoss: 0.461631\n",
      "Train Epoch: 80 [640/1248 (51%)]\tLoss: 0.666419\n",
      "Train Epoch: 80 [960/1248 (77%)]\tLoss: 0.699074\n",
      "Train Epoch: 81 [0/1248 (0%)]\tLoss: 0.548052\n",
      "Train Epoch: 81 [320/1248 (26%)]\tLoss: 0.454405\n",
      "Train Epoch: 81 [640/1248 (51%)]\tLoss: 0.668741\n",
      "Train Epoch: 81 [960/1248 (77%)]\tLoss: 0.709046\n",
      "Train Epoch: 82 [0/1248 (0%)]\tLoss: 0.572510\n",
      "Train Epoch: 82 [320/1248 (26%)]\tLoss: 0.458701\n",
      "Train Epoch: 82 [640/1248 (51%)]\tLoss: 0.668863\n",
      "Train Epoch: 82 [960/1248 (77%)]\tLoss: 0.709481\n",
      "Train Epoch: 83 [0/1248 (0%)]\tLoss: 0.567074\n",
      "Train Epoch: 83 [320/1248 (26%)]\tLoss: 0.457596\n",
      "Train Epoch: 83 [640/1248 (51%)]\tLoss: 0.669623\n",
      "Train Epoch: 83 [960/1248 (77%)]\tLoss: 0.700311\n",
      "Train Epoch: 84 [0/1248 (0%)]\tLoss: 0.549051\n",
      "Train Epoch: 84 [320/1248 (26%)]\tLoss: 0.472593\n",
      "Train Epoch: 84 [640/1248 (51%)]\tLoss: 0.666592\n",
      "Train Epoch: 84 [960/1248 (77%)]\tLoss: 0.711185\n",
      "Train Epoch: 85 [0/1248 (0%)]\tLoss: 0.545875\n",
      "Train Epoch: 85 [320/1248 (26%)]\tLoss: 0.471779\n",
      "Train Epoch: 85 [640/1248 (51%)]\tLoss: 0.663799\n",
      "Train Epoch: 85 [960/1248 (77%)]\tLoss: 0.710053\n",
      "Train Epoch: 86 [0/1248 (0%)]\tLoss: 0.587408\n",
      "Train Epoch: 86 [320/1248 (26%)]\tLoss: 0.470777\n",
      "Train Epoch: 86 [640/1248 (51%)]\tLoss: 0.664914\n",
      "Train Epoch: 86 [960/1248 (77%)]\tLoss: 0.691703\n",
      "Train Epoch: 87 [0/1248 (0%)]\tLoss: 0.592568\n",
      "Train Epoch: 87 [320/1248 (26%)]\tLoss: 0.467103\n",
      "Train Epoch: 87 [640/1248 (51%)]\tLoss: 0.665043\n",
      "Train Epoch: 87 [960/1248 (77%)]\tLoss: 0.690105\n",
      "Train Epoch: 88 [0/1248 (0%)]\tLoss: 0.548282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 88 [320/1248 (26%)]\tLoss: 0.465556\n",
      "Train Epoch: 88 [640/1248 (51%)]\tLoss: 0.704509\n",
      "Train Epoch: 88 [960/1248 (77%)]\tLoss: 0.689972\n",
      "Train Epoch: 89 [0/1248 (0%)]\tLoss: 0.579005\n",
      "Train Epoch: 89 [320/1248 (26%)]\tLoss: 0.480100\n",
      "Train Epoch: 89 [640/1248 (51%)]\tLoss: 0.671959\n",
      "Train Epoch: 89 [960/1248 (77%)]\tLoss: 0.690455\n",
      "Train Epoch: 90 [0/1248 (0%)]\tLoss: 0.588700\n",
      "Train Epoch: 90 [320/1248 (26%)]\tLoss: 0.449879\n",
      "Train Epoch: 90 [640/1248 (51%)]\tLoss: 0.676757\n",
      "Train Epoch: 90 [960/1248 (77%)]\tLoss: 0.706111\n",
      "Train Epoch: 91 [0/1248 (0%)]\tLoss: 0.563778\n",
      "Train Epoch: 91 [320/1248 (26%)]\tLoss: 0.452484\n",
      "Train Epoch: 91 [640/1248 (51%)]\tLoss: 0.697880\n",
      "Train Epoch: 91 [960/1248 (77%)]\tLoss: 0.692543\n",
      "Train Epoch: 92 [0/1248 (0%)]\tLoss: 0.551724\n",
      "Train Epoch: 92 [320/1248 (26%)]\tLoss: 0.450272\n",
      "Train Epoch: 92 [640/1248 (51%)]\tLoss: 0.701271\n",
      "Train Epoch: 92 [960/1248 (77%)]\tLoss: 0.700063\n",
      "Train Epoch: 93 [0/1248 (0%)]\tLoss: 0.563805\n",
      "Train Epoch: 93 [320/1248 (26%)]\tLoss: 0.454537\n",
      "Train Epoch: 93 [640/1248 (51%)]\tLoss: 0.677443\n",
      "Train Epoch: 93 [960/1248 (77%)]\tLoss: 0.710404\n",
      "Train Epoch: 94 [0/1248 (0%)]\tLoss: 0.548994\n",
      "Train Epoch: 94 [320/1248 (26%)]\tLoss: 0.456948\n",
      "Train Epoch: 94 [640/1248 (51%)]\tLoss: 0.670592\n",
      "Train Epoch: 94 [960/1248 (77%)]\tLoss: 0.694080\n",
      "Train Epoch: 95 [0/1248 (0%)]\tLoss: 0.549403\n",
      "Train Epoch: 95 [320/1248 (26%)]\tLoss: 0.471768\n",
      "Train Epoch: 95 [640/1248 (51%)]\tLoss: 0.667412\n",
      "Train Epoch: 95 [960/1248 (77%)]\tLoss: 0.693914\n",
      "Train Epoch: 96 [0/1248 (0%)]\tLoss: 0.575775\n",
      "Train Epoch: 96 [320/1248 (26%)]\tLoss: 0.452342\n",
      "Train Epoch: 96 [640/1248 (51%)]\tLoss: 0.678769\n",
      "Train Epoch: 96 [960/1248 (77%)]\tLoss: 0.708636\n",
      "Train Epoch: 97 [0/1248 (0%)]\tLoss: 0.550293\n",
      "Train Epoch: 97 [320/1248 (26%)]\tLoss: 0.457618\n",
      "Train Epoch: 97 [640/1248 (51%)]\tLoss: 0.669948\n",
      "Train Epoch: 97 [960/1248 (77%)]\tLoss: 0.694380\n",
      "Train Epoch: 98 [0/1248 (0%)]\tLoss: 0.577473\n",
      "Train Epoch: 98 [320/1248 (26%)]\tLoss: 0.478447\n",
      "Train Epoch: 98 [640/1248 (51%)]\tLoss: 0.694452\n",
      "Train Epoch: 98 [960/1248 (77%)]\tLoss: 0.713268\n",
      "Train Epoch: 99 [0/1248 (0%)]\tLoss: 0.557897\n",
      "Train Epoch: 99 [320/1248 (26%)]\tLoss: 0.452656\n",
      "Train Epoch: 99 [640/1248 (51%)]\tLoss: 0.670133\n",
      "Train Epoch: 99 [960/1248 (77%)]\tLoss: 0.689863\n",
      "Train Epoch: 100 [0/1248 (0%)]\tLoss: 0.579502\n",
      "Train Epoch: 100 [320/1248 (26%)]\tLoss: 0.480753\n",
      "Train Epoch: 100 [640/1248 (51%)]\tLoss: 0.668161\n",
      "Train Epoch: 100 [960/1248 (77%)]\tLoss: 0.696396\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model3.parameters(), lr=0.03)  \n",
    "epoch = 100\n",
    "for epoch in range(1, epoch + 1):\n",
    "    train(model3, federated_train_loader, optimizer, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e661a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "import torch \n",
    "torch.save(model3.state_dict(), './model/master_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5cfdfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18a01d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "row =  [22,1024,0,0.472,0.034,360,736,24,2,869,4,1,0,3,0,26]\n",
    "client = predict(row, model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99e68aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[4.1694155 5.2498083 3.6201644 4.387382  4.3095016]]\n"
     ]
    }
   ],
   "source": [
    "print('Predicted:', client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "382d2428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(path, header=None)\n",
    "\n",
    "        \n",
    "        df.rename(columns=df.iloc[0]).drop(df.index[0])\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[1:, 0:16].astype('float32')\n",
    "        self.y = df.values[1:, 16:21].astype('float32')\n",
    "    \n",
    "        # ensure target has the right shape\n",
    "        self.y = self.y.reshape((len(self.y), 5))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=False)\n",
    "    test_dl = DataLoader(test, batch_size=32, shuffle=False)\n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fbf5fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7ffc154b6e50>\n"
     ]
    }
   ],
   "source": [
    "print(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2373e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch mlp for regression\n",
    "from numpy import vstack\n",
    "from numpy import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        output = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        output = output.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 5))\n",
    "        # store\n",
    "        predictions.append(output)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate mse\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08a53093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.315, RMSE: 1.147\n"
     ]
    }
   ],
   "source": [
    "path = './data/qoe/federated_devices/pokemon_complet.csv'\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "\n",
    "mse = evaluate_model(test_dl, model3)\n",
    "print('MSE: %.3f, RMSE: %.3f' % (mse, sqrt(mse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600f2a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e81ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
