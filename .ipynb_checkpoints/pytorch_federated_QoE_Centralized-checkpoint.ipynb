{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf0f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch mlp for regression\n",
    "from numpy import vstack\n",
    "from numpy import sqrt\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, Sequential\n",
    "from torch.nn import Sigmoid , ReLU\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b65866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(path, header=None)\n",
    "       # df.rename(columns=df.iloc[0]).drop(df.index[0])\n",
    "        df.drop_duplicates(keep=False,inplace=True)\n",
    "  \n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[1:, 0:12].astype('float32')\n",
    "        self.y = df.values[1:, 12:14].astype('float32')\n",
    "    \n",
    "        # ensure target has the right shape\n",
    "        self.y = self.y.reshape((len(self.y), 2))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.2):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 12)\n",
    "        #xavier_uniform_(self.hidden1.weight)\n",
    "        self.act1 = Sigmoid()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(12, 6)\n",
    "        #xavier_uniform_(self.hidden2.weight)\n",
    "        self.act2 = Sigmoid()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(6, 2)\n",
    "        #xavier_uniform_(self.hidden3.weight)\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "         #input to first hidden layer\n",
    "         X = self.hidden1(X)\n",
    "         X = self.act1(X)\n",
    "         # second hidden layer\n",
    "         X = self.hidden2(X)\n",
    "         X = self.act2(X)\n",
    "         # third hidden layer and output\n",
    "         X = self.hidden3(X)\n",
    "         return X\n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    \n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=1, shuffle=False)\n",
    "    test_dl = DataLoader(test, batch_size=1, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    "\n",
    "\n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "\n",
    "    running_loss=0\n",
    "    correct=0\n",
    "    total=0\n",
    "    train_accu = []\n",
    "    test_loss = 0\n",
    "    train_losses = []\n",
    "    # define the optimization\n",
    "    criterion = MSELoss()\n",
    "    #loss_fn=nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "   \n",
    "    # enumerate epochs\n",
    "    for epoch in range(200):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            test_loss += loss\n",
    "            #loss=loss_fn(yhat,targets)\n",
    "            running_loss += loss.item()\n",
    "            history = loss.item\n",
    "            total += targets.size(0)\n",
    "            #print(loss)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            _, predicted = yhat.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                #print(f'Train Epoch: {epoch} [{batch_idx*32}/{len(federate_train_loader)*32} ({100. * batch_idx / len(federated_train_loader)}%)] \\t Loss: {loss.item()}')\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, \n",
    "                        i * 1 , # no of images done\n",
    "                        len(train_dl) * 1, # total images left\n",
    "                        100. * i / len(train_dl), \n",
    "                        loss.item()\n",
    "                     )\n",
    "                )\n",
    "          \n",
    "        train_loss=running_loss/len(train_dl)\n",
    "        accu=100.*correct/total\n",
    "        train_accu.append(accu)\n",
    "        train_losses.append(train_loss)\n",
    "    test_loss /= len(train_dl)\n",
    "    print('Train Loss: %.3f | Accuracy: %.3f'%(train_loss,accu))\n",
    "            \n",
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        output = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        output = output.detach().numpy()\n",
    "\n",
    "\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 2))\n",
    "        # store\n",
    "        predictions.append(output)\n",
    "        actuals.append(actual)\n",
    "     \n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate mse\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    #final_train_acc = train_acc/len(test_dl)\n",
    "    #acc = accuracy_score(actuals, predictions)\n",
    "    #print(acc)\n",
    "    return mse\n",
    "\n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "178635e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246 62\n",
      "Train Epoch: 0 [0/246 (0%)]\tLoss: 7.582303\n",
      "Train Epoch: 0 [100/246 (41%)]\tLoss: 0.085501\n",
      "Train Epoch: 0 [200/246 (81%)]\tLoss: 0.085274\n",
      "Train Epoch: 1 [0/246 (0%)]\tLoss: 1.589953\n",
      "Train Epoch: 1 [100/246 (41%)]\tLoss: 0.080303\n",
      "Train Epoch: 1 [200/246 (81%)]\tLoss: 0.085048\n",
      "Train Epoch: 2 [0/246 (0%)]\tLoss: 1.605685\n",
      "Train Epoch: 2 [100/246 (41%)]\tLoss: 0.079630\n",
      "Train Epoch: 2 [200/246 (81%)]\tLoss: 0.084846\n",
      "Train Epoch: 3 [0/246 (0%)]\tLoss: 1.623642\n",
      "Train Epoch: 3 [100/246 (41%)]\tLoss: 0.078802\n",
      "Train Epoch: 3 [200/246 (81%)]\tLoss: 0.084593\n",
      "Train Epoch: 4 [0/246 (0%)]\tLoss: 1.656591\n",
      "Train Epoch: 4 [100/246 (41%)]\tLoss: 0.077351\n",
      "Train Epoch: 4 [200/246 (81%)]\tLoss: 0.083657\n",
      "Train Epoch: 5 [0/246 (0%)]\tLoss: 1.754469\n",
      "Train Epoch: 5 [100/246 (41%)]\tLoss: 0.073408\n",
      "Train Epoch: 5 [200/246 (81%)]\tLoss: 0.076276\n",
      "Train Epoch: 6 [0/246 (0%)]\tLoss: 2.005519\n",
      "Train Epoch: 6 [100/246 (41%)]\tLoss: 0.067611\n",
      "Train Epoch: 6 [200/246 (81%)]\tLoss: 0.067616\n",
      "Train Epoch: 7 [0/246 (0%)]\tLoss: 2.065365\n",
      "Train Epoch: 7 [100/246 (41%)]\tLoss: 0.065721\n",
      "Train Epoch: 7 [200/246 (81%)]\tLoss: 0.065308\n",
      "Train Epoch: 8 [0/246 (0%)]\tLoss: 2.071855\n",
      "Train Epoch: 8 [100/246 (41%)]\tLoss: 0.065239\n",
      "Train Epoch: 8 [200/246 (81%)]\tLoss: 0.064546\n",
      "Train Epoch: 9 [0/246 (0%)]\tLoss: 2.073732\n",
      "Train Epoch: 9 [100/246 (41%)]\tLoss: 0.065056\n",
      "Train Epoch: 9 [200/246 (81%)]\tLoss: 0.064211\n",
      "Train Epoch: 10 [0/246 (0%)]\tLoss: 2.074584\n",
      "Train Epoch: 10 [100/246 (41%)]\tLoss: 0.064969\n",
      "Train Epoch: 10 [200/246 (81%)]\tLoss: 0.064045\n",
      "Train Epoch: 11 [0/246 (0%)]\tLoss: 2.075045\n",
      "Train Epoch: 11 [100/246 (41%)]\tLoss: 0.064923\n",
      "Train Epoch: 11 [200/246 (81%)]\tLoss: 0.063957\n",
      "Train Epoch: 12 [0/246 (0%)]\tLoss: 2.075310\n",
      "Train Epoch: 12 [100/246 (41%)]\tLoss: 0.064896\n",
      "Train Epoch: 12 [200/246 (81%)]\tLoss: 0.063911\n",
      "Train Epoch: 13 [0/246 (0%)]\tLoss: 2.075460\n",
      "Train Epoch: 13 [100/246 (41%)]\tLoss: 0.064880\n",
      "Train Epoch: 13 [200/246 (81%)]\tLoss: 0.063890\n",
      "Train Epoch: 14 [0/246 (0%)]\tLoss: 2.075538\n",
      "Train Epoch: 14 [100/246 (41%)]\tLoss: 0.064870\n",
      "Train Epoch: 14 [200/246 (81%)]\tLoss: 0.063883\n",
      "Train Epoch: 15 [0/246 (0%)]\tLoss: 2.075568\n",
      "Train Epoch: 15 [100/246 (41%)]\tLoss: 0.064865\n",
      "Train Epoch: 15 [200/246 (81%)]\tLoss: 0.063885\n",
      "Train Epoch: 16 [0/246 (0%)]\tLoss: 2.075566\n",
      "Train Epoch: 16 [100/246 (41%)]\tLoss: 0.064862\n",
      "Train Epoch: 16 [200/246 (81%)]\tLoss: 0.063894\n",
      "Train Epoch: 17 [0/246 (0%)]\tLoss: 2.075540\n",
      "Train Epoch: 17 [100/246 (41%)]\tLoss: 0.064860\n",
      "Train Epoch: 17 [200/246 (81%)]\tLoss: 0.063905\n",
      "Train Epoch: 18 [0/246 (0%)]\tLoss: 2.075500\n",
      "Train Epoch: 18 [100/246 (41%)]\tLoss: 0.064860\n",
      "Train Epoch: 18 [200/246 (81%)]\tLoss: 0.063920\n",
      "Train Epoch: 19 [0/246 (0%)]\tLoss: 2.075449\n",
      "Train Epoch: 19 [100/246 (41%)]\tLoss: 0.064860\n",
      "Train Epoch: 19 [200/246 (81%)]\tLoss: 0.063935\n",
      "Train Epoch: 20 [0/246 (0%)]\tLoss: 2.075395\n",
      "Train Epoch: 20 [100/246 (41%)]\tLoss: 0.064861\n",
      "Train Epoch: 20 [200/246 (81%)]\tLoss: 0.063952\n",
      "Train Epoch: 21 [0/246 (0%)]\tLoss: 2.075336\n",
      "Train Epoch: 21 [100/246 (41%)]\tLoss: 0.064863\n",
      "Train Epoch: 21 [200/246 (81%)]\tLoss: 0.063969\n",
      "Train Epoch: 22 [0/246 (0%)]\tLoss: 2.075275\n",
      "Train Epoch: 22 [100/246 (41%)]\tLoss: 0.064864\n",
      "Train Epoch: 22 [200/246 (81%)]\tLoss: 0.063986\n",
      "Train Epoch: 23 [0/246 (0%)]\tLoss: 2.075213\n",
      "Train Epoch: 23 [100/246 (41%)]\tLoss: 0.064866\n",
      "Train Epoch: 23 [200/246 (81%)]\tLoss: 0.064003\n",
      "Train Epoch: 24 [0/246 (0%)]\tLoss: 2.075150\n",
      "Train Epoch: 24 [100/246 (41%)]\tLoss: 0.064867\n",
      "Train Epoch: 24 [200/246 (81%)]\tLoss: 0.064020\n",
      "Train Epoch: 25 [0/246 (0%)]\tLoss: 2.075091\n",
      "Train Epoch: 25 [100/246 (41%)]\tLoss: 0.064869\n",
      "Train Epoch: 25 [200/246 (81%)]\tLoss: 0.064037\n",
      "Train Epoch: 26 [0/246 (0%)]\tLoss: 2.075030\n",
      "Train Epoch: 26 [100/246 (41%)]\tLoss: 0.064870\n",
      "Train Epoch: 26 [200/246 (81%)]\tLoss: 0.064053\n",
      "Train Epoch: 27 [0/246 (0%)]\tLoss: 2.074975\n",
      "Train Epoch: 27 [100/246 (41%)]\tLoss: 0.064872\n",
      "Train Epoch: 27 [200/246 (81%)]\tLoss: 0.064069\n",
      "Train Epoch: 28 [0/246 (0%)]\tLoss: 2.074920\n",
      "Train Epoch: 28 [100/246 (41%)]\tLoss: 0.064873\n",
      "Train Epoch: 28 [200/246 (81%)]\tLoss: 0.064084\n",
      "Train Epoch: 29 [0/246 (0%)]\tLoss: 2.074870\n",
      "Train Epoch: 29 [100/246 (41%)]\tLoss: 0.064874\n",
      "Train Epoch: 29 [200/246 (81%)]\tLoss: 0.064099\n",
      "Train Epoch: 30 [0/246 (0%)]\tLoss: 2.074819\n",
      "Train Epoch: 30 [100/246 (41%)]\tLoss: 0.064876\n",
      "Train Epoch: 30 [200/246 (81%)]\tLoss: 0.064113\n",
      "Train Epoch: 31 [0/246 (0%)]\tLoss: 2.074772\n",
      "Train Epoch: 31 [100/246 (41%)]\tLoss: 0.064876\n",
      "Train Epoch: 31 [200/246 (81%)]\tLoss: 0.064127\n",
      "Train Epoch: 32 [0/246 (0%)]\tLoss: 2.074728\n",
      "Train Epoch: 32 [100/246 (41%)]\tLoss: 0.064877\n",
      "Train Epoch: 32 [200/246 (81%)]\tLoss: 0.064141\n",
      "Train Epoch: 33 [0/246 (0%)]\tLoss: 2.074687\n",
      "Train Epoch: 33 [100/246 (41%)]\tLoss: 0.064878\n",
      "Train Epoch: 33 [200/246 (81%)]\tLoss: 0.064153\n",
      "Train Epoch: 34 [0/246 (0%)]\tLoss: 2.074649\n",
      "Train Epoch: 34 [100/246 (41%)]\tLoss: 0.064878\n",
      "Train Epoch: 34 [200/246 (81%)]\tLoss: 0.064166\n",
      "Train Epoch: 35 [0/246 (0%)]\tLoss: 2.074611\n",
      "Train Epoch: 35 [100/246 (41%)]\tLoss: 0.064878\n",
      "Train Epoch: 35 [200/246 (81%)]\tLoss: 0.064177\n",
      "Train Epoch: 36 [0/246 (0%)]\tLoss: 2.074577\n",
      "Train Epoch: 36 [100/246 (41%)]\tLoss: 0.064878\n",
      "Train Epoch: 36 [200/246 (81%)]\tLoss: 0.064189\n",
      "Train Epoch: 37 [0/246 (0%)]\tLoss: 2.074547\n",
      "Train Epoch: 37 [100/246 (41%)]\tLoss: 0.064878\n",
      "Train Epoch: 37 [200/246 (81%)]\tLoss: 0.064199\n",
      "Train Epoch: 38 [0/246 (0%)]\tLoss: 2.074516\n",
      "Train Epoch: 38 [100/246 (41%)]\tLoss: 0.064878\n",
      "Train Epoch: 38 [200/246 (81%)]\tLoss: 0.064210\n",
      "Train Epoch: 39 [0/246 (0%)]\tLoss: 2.074488\n",
      "Train Epoch: 39 [100/246 (41%)]\tLoss: 0.064878\n",
      "Train Epoch: 39 [200/246 (81%)]\tLoss: 0.064220\n",
      "Train Epoch: 40 [0/246 (0%)]\tLoss: 2.074463\n",
      "Train Epoch: 40 [100/246 (41%)]\tLoss: 0.064877\n",
      "Train Epoch: 40 [200/246 (81%)]\tLoss: 0.064229\n",
      "Train Epoch: 41 [0/246 (0%)]\tLoss: 2.074440\n",
      "Train Epoch: 41 [100/246 (41%)]\tLoss: 0.064876\n",
      "Train Epoch: 41 [200/246 (81%)]\tLoss: 0.064238\n",
      "Train Epoch: 42 [0/246 (0%)]\tLoss: 2.074417\n",
      "Train Epoch: 42 [100/246 (41%)]\tLoss: 0.064875\n",
      "Train Epoch: 42 [200/246 (81%)]\tLoss: 0.064247\n",
      "Train Epoch: 43 [0/246 (0%)]\tLoss: 2.074399\n",
      "Train Epoch: 43 [100/246 (41%)]\tLoss: 0.064874\n",
      "Train Epoch: 43 [200/246 (81%)]\tLoss: 0.064255\n",
      "Train Epoch: 44 [0/246 (0%)]\tLoss: 2.074381\n",
      "Train Epoch: 44 [100/246 (41%)]\tLoss: 0.064872\n",
      "Train Epoch: 44 [200/246 (81%)]\tLoss: 0.064263\n",
      "Train Epoch: 45 [0/246 (0%)]\tLoss: 2.074363\n",
      "Train Epoch: 45 [100/246 (41%)]\tLoss: 0.064871\n",
      "Train Epoch: 45 [200/246 (81%)]\tLoss: 0.064271\n",
      "Train Epoch: 46 [0/246 (0%)]\tLoss: 2.074347\n",
      "Train Epoch: 46 [100/246 (41%)]\tLoss: 0.064869\n",
      "Train Epoch: 46 [200/246 (81%)]\tLoss: 0.064278\n",
      "Train Epoch: 47 [0/246 (0%)]\tLoss: 2.074333\n",
      "Train Epoch: 47 [100/246 (41%)]\tLoss: 0.064867\n",
      "Train Epoch: 47 [200/246 (81%)]\tLoss: 0.064285\n",
      "Train Epoch: 48 [0/246 (0%)]\tLoss: 2.074323\n",
      "Train Epoch: 48 [100/246 (41%)]\tLoss: 0.064865\n",
      "Train Epoch: 48 [200/246 (81%)]\tLoss: 0.064292\n",
      "Train Epoch: 49 [0/246 (0%)]\tLoss: 2.074314\n",
      "Train Epoch: 49 [100/246 (41%)]\tLoss: 0.064863\n",
      "Train Epoch: 49 [200/246 (81%)]\tLoss: 0.064298\n",
      "Train Epoch: 50 [0/246 (0%)]\tLoss: 2.074303\n",
      "Train Epoch: 50 [100/246 (41%)]\tLoss: 0.064861\n",
      "Train Epoch: 50 [200/246 (81%)]\tLoss: 0.064305\n",
      "Train Epoch: 51 [0/246 (0%)]\tLoss: 2.074295\n",
      "Train Epoch: 51 [100/246 (41%)]\tLoss: 0.064858\n",
      "Train Epoch: 51 [200/246 (81%)]\tLoss: 0.064310\n",
      "Train Epoch: 52 [0/246 (0%)]\tLoss: 2.074290\n",
      "Train Epoch: 52 [100/246 (41%)]\tLoss: 0.064856\n",
      "Train Epoch: 52 [200/246 (81%)]\tLoss: 0.064316\n",
      "Train Epoch: 53 [0/246 (0%)]\tLoss: 2.074282\n",
      "Train Epoch: 53 [100/246 (41%)]\tLoss: 0.064853\n",
      "Train Epoch: 53 [200/246 (81%)]\tLoss: 0.064322\n",
      "Train Epoch: 54 [0/246 (0%)]\tLoss: 2.074278\n",
      "Train Epoch: 54 [100/246 (41%)]\tLoss: 0.064850\n",
      "Train Epoch: 54 [200/246 (81%)]\tLoss: 0.064327\n",
      "Train Epoch: 55 [0/246 (0%)]\tLoss: 2.074274\n",
      "Train Epoch: 55 [100/246 (41%)]\tLoss: 0.064847\n",
      "Train Epoch: 55 [200/246 (81%)]\tLoss: 0.064332\n",
      "Train Epoch: 56 [0/246 (0%)]\tLoss: 2.074272\n",
      "Train Epoch: 56 [100/246 (41%)]\tLoss: 0.064844\n",
      "Train Epoch: 56 [200/246 (81%)]\tLoss: 0.064336\n",
      "Train Epoch: 57 [0/246 (0%)]\tLoss: 2.074269\n",
      "Train Epoch: 57 [100/246 (41%)]\tLoss: 0.064840\n",
      "Train Epoch: 57 [200/246 (81%)]\tLoss: 0.064341\n",
      "Train Epoch: 58 [0/246 (0%)]\tLoss: 2.074268\n",
      "Train Epoch: 58 [100/246 (41%)]\tLoss: 0.064837\n",
      "Train Epoch: 58 [200/246 (81%)]\tLoss: 0.064345\n",
      "Train Epoch: 59 [0/246 (0%)]\tLoss: 2.074270\n",
      "Train Epoch: 59 [100/246 (41%)]\tLoss: 0.064833\n",
      "Train Epoch: 59 [200/246 (81%)]\tLoss: 0.064350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 60 [0/246 (0%)]\tLoss: 2.074271\n",
      "Train Epoch: 60 [100/246 (41%)]\tLoss: 0.064829\n",
      "Train Epoch: 60 [200/246 (81%)]\tLoss: 0.064354\n",
      "Train Epoch: 61 [0/246 (0%)]\tLoss: 2.074271\n",
      "Train Epoch: 61 [100/246 (41%)]\tLoss: 0.064825\n",
      "Train Epoch: 61 [200/246 (81%)]\tLoss: 0.064357\n",
      "Train Epoch: 62 [0/246 (0%)]\tLoss: 2.074274\n",
      "Train Epoch: 62 [100/246 (41%)]\tLoss: 0.064821\n",
      "Train Epoch: 62 [200/246 (81%)]\tLoss: 0.064361\n",
      "Train Epoch: 63 [0/246 (0%)]\tLoss: 2.074279\n",
      "Train Epoch: 63 [100/246 (41%)]\tLoss: 0.064817\n",
      "Train Epoch: 63 [200/246 (81%)]\tLoss: 0.064365\n",
      "Train Epoch: 64 [0/246 (0%)]\tLoss: 2.074282\n",
      "Train Epoch: 64 [100/246 (41%)]\tLoss: 0.064813\n",
      "Train Epoch: 64 [200/246 (81%)]\tLoss: 0.064368\n",
      "Train Epoch: 65 [0/246 (0%)]\tLoss: 2.074286\n",
      "Train Epoch: 65 [100/246 (41%)]\tLoss: 0.064808\n",
      "Train Epoch: 65 [200/246 (81%)]\tLoss: 0.064371\n",
      "Train Epoch: 66 [0/246 (0%)]\tLoss: 2.074293\n",
      "Train Epoch: 66 [100/246 (41%)]\tLoss: 0.064804\n",
      "Train Epoch: 66 [200/246 (81%)]\tLoss: 0.064375\n",
      "Train Epoch: 67 [0/246 (0%)]\tLoss: 2.074299\n",
      "Train Epoch: 67 [100/246 (41%)]\tLoss: 0.064799\n",
      "Train Epoch: 67 [200/246 (81%)]\tLoss: 0.064377\n",
      "Train Epoch: 68 [0/246 (0%)]\tLoss: 2.074304\n",
      "Train Epoch: 68 [100/246 (41%)]\tLoss: 0.064794\n",
      "Train Epoch: 68 [200/246 (81%)]\tLoss: 0.064380\n",
      "Train Epoch: 69 [0/246 (0%)]\tLoss: 2.074311\n",
      "Train Epoch: 69 [100/246 (41%)]\tLoss: 0.064789\n",
      "Train Epoch: 69 [200/246 (81%)]\tLoss: 0.064383\n",
      "Train Epoch: 70 [0/246 (0%)]\tLoss: 2.074318\n",
      "Train Epoch: 70 [100/246 (41%)]\tLoss: 0.064784\n",
      "Train Epoch: 70 [200/246 (81%)]\tLoss: 0.064386\n",
      "Train Epoch: 71 [0/246 (0%)]\tLoss: 2.074327\n",
      "Train Epoch: 71 [100/246 (41%)]\tLoss: 0.064779\n",
      "Train Epoch: 71 [200/246 (81%)]\tLoss: 0.064388\n",
      "Train Epoch: 72 [0/246 (0%)]\tLoss: 2.074336\n",
      "Train Epoch: 72 [100/246 (41%)]\tLoss: 0.064774\n",
      "Train Epoch: 72 [200/246 (81%)]\tLoss: 0.064391\n",
      "Train Epoch: 73 [0/246 (0%)]\tLoss: 2.074345\n",
      "Train Epoch: 73 [100/246 (41%)]\tLoss: 0.064768\n",
      "Train Epoch: 73 [200/246 (81%)]\tLoss: 0.064393\n",
      "Train Epoch: 74 [0/246 (0%)]\tLoss: 2.074356\n",
      "Train Epoch: 74 [100/246 (41%)]\tLoss: 0.064763\n",
      "Train Epoch: 74 [200/246 (81%)]\tLoss: 0.064395\n",
      "Train Epoch: 75 [0/246 (0%)]\tLoss: 2.074365\n",
      "Train Epoch: 75 [100/246 (41%)]\tLoss: 0.064757\n",
      "Train Epoch: 75 [200/246 (81%)]\tLoss: 0.064397\n",
      "Train Epoch: 76 [0/246 (0%)]\tLoss: 2.074376\n",
      "Train Epoch: 76 [100/246 (41%)]\tLoss: 0.064751\n",
      "Train Epoch: 76 [200/246 (81%)]\tLoss: 0.064399\n",
      "Train Epoch: 77 [0/246 (0%)]\tLoss: 2.074387\n",
      "Train Epoch: 77 [100/246 (41%)]\tLoss: 0.064746\n",
      "Train Epoch: 77 [200/246 (81%)]\tLoss: 0.064401\n",
      "Train Epoch: 78 [0/246 (0%)]\tLoss: 2.074399\n",
      "Train Epoch: 78 [100/246 (41%)]\tLoss: 0.064740\n",
      "Train Epoch: 78 [200/246 (81%)]\tLoss: 0.064403\n",
      "Train Epoch: 79 [0/246 (0%)]\tLoss: 2.074410\n",
      "Train Epoch: 79 [100/246 (41%)]\tLoss: 0.064734\n",
      "Train Epoch: 79 [200/246 (81%)]\tLoss: 0.064405\n",
      "Train Epoch: 80 [0/246 (0%)]\tLoss: 2.074421\n",
      "Train Epoch: 80 [100/246 (41%)]\tLoss: 0.064728\n",
      "Train Epoch: 80 [200/246 (81%)]\tLoss: 0.064406\n",
      "Train Epoch: 81 [0/246 (0%)]\tLoss: 2.074434\n",
      "Train Epoch: 81 [100/246 (41%)]\tLoss: 0.064721\n",
      "Train Epoch: 81 [200/246 (81%)]\tLoss: 0.064408\n",
      "Train Epoch: 82 [0/246 (0%)]\tLoss: 2.074447\n",
      "Train Epoch: 82 [100/246 (41%)]\tLoss: 0.064715\n",
      "Train Epoch: 82 [200/246 (81%)]\tLoss: 0.064409\n",
      "Train Epoch: 83 [0/246 (0%)]\tLoss: 2.074460\n",
      "Train Epoch: 83 [100/246 (41%)]\tLoss: 0.064709\n",
      "Train Epoch: 83 [200/246 (81%)]\tLoss: 0.064410\n",
      "Train Epoch: 84 [0/246 (0%)]\tLoss: 2.074474\n",
      "Train Epoch: 84 [100/246 (41%)]\tLoss: 0.064702\n",
      "Train Epoch: 84 [200/246 (81%)]\tLoss: 0.064412\n",
      "Train Epoch: 85 [0/246 (0%)]\tLoss: 2.074488\n",
      "Train Epoch: 85 [100/246 (41%)]\tLoss: 0.064696\n",
      "Train Epoch: 85 [200/246 (81%)]\tLoss: 0.064413\n",
      "Train Epoch: 86 [0/246 (0%)]\tLoss: 2.074502\n",
      "Train Epoch: 86 [100/246 (41%)]\tLoss: 0.064689\n",
      "Train Epoch: 86 [200/246 (81%)]\tLoss: 0.064414\n",
      "Train Epoch: 87 [0/246 (0%)]\tLoss: 2.074517\n",
      "Train Epoch: 87 [100/246 (41%)]\tLoss: 0.064683\n",
      "Train Epoch: 87 [200/246 (81%)]\tLoss: 0.064415\n",
      "Train Epoch: 88 [0/246 (0%)]\tLoss: 2.074531\n",
      "Train Epoch: 88 [100/246 (41%)]\tLoss: 0.064676\n",
      "Train Epoch: 88 [200/246 (81%)]\tLoss: 0.064415\n",
      "Train Epoch: 89 [0/246 (0%)]\tLoss: 2.074547\n",
      "Train Epoch: 89 [100/246 (41%)]\tLoss: 0.064669\n",
      "Train Epoch: 89 [200/246 (81%)]\tLoss: 0.064416\n",
      "Train Epoch: 90 [0/246 (0%)]\tLoss: 2.074564\n",
      "Train Epoch: 90 [100/246 (41%)]\tLoss: 0.064662\n",
      "Train Epoch: 90 [200/246 (81%)]\tLoss: 0.064417\n",
      "Train Epoch: 91 [0/246 (0%)]\tLoss: 2.074578\n",
      "Train Epoch: 91 [100/246 (41%)]\tLoss: 0.064655\n",
      "Train Epoch: 91 [200/246 (81%)]\tLoss: 0.064417\n",
      "Train Epoch: 92 [0/246 (0%)]\tLoss: 2.074594\n",
      "Train Epoch: 92 [100/246 (41%)]\tLoss: 0.064648\n",
      "Train Epoch: 92 [200/246 (81%)]\tLoss: 0.064418\n",
      "Train Epoch: 93 [0/246 (0%)]\tLoss: 2.074609\n",
      "Train Epoch: 93 [100/246 (41%)]\tLoss: 0.064641\n",
      "Train Epoch: 93 [200/246 (81%)]\tLoss: 0.064418\n",
      "Train Epoch: 94 [0/246 (0%)]\tLoss: 2.074626\n",
      "Train Epoch: 94 [100/246 (41%)]\tLoss: 0.064634\n",
      "Train Epoch: 94 [200/246 (81%)]\tLoss: 0.064418\n",
      "Train Epoch: 95 [0/246 (0%)]\tLoss: 2.074642\n",
      "Train Epoch: 95 [100/246 (41%)]\tLoss: 0.064627\n",
      "Train Epoch: 95 [200/246 (81%)]\tLoss: 0.064418\n",
      "Train Epoch: 96 [0/246 (0%)]\tLoss: 2.074660\n",
      "Train Epoch: 96 [100/246 (41%)]\tLoss: 0.064620\n",
      "Train Epoch: 96 [200/246 (81%)]\tLoss: 0.064419\n",
      "Train Epoch: 97 [0/246 (0%)]\tLoss: 2.074675\n",
      "Train Epoch: 97 [100/246 (41%)]\tLoss: 0.064612\n",
      "Train Epoch: 97 [200/246 (81%)]\tLoss: 0.064419\n",
      "Train Epoch: 98 [0/246 (0%)]\tLoss: 2.074693\n",
      "Train Epoch: 98 [100/246 (41%)]\tLoss: 0.064605\n",
      "Train Epoch: 98 [200/246 (81%)]\tLoss: 0.064418\n",
      "Train Epoch: 99 [0/246 (0%)]\tLoss: 2.074711\n",
      "Train Epoch: 99 [100/246 (41%)]\tLoss: 0.064598\n",
      "Train Epoch: 99 [200/246 (81%)]\tLoss: 0.064418\n",
      "Train Epoch: 100 [0/246 (0%)]\tLoss: 2.074728\n",
      "Train Epoch: 100 [100/246 (41%)]\tLoss: 0.064590\n",
      "Train Epoch: 100 [200/246 (81%)]\tLoss: 0.064418\n",
      "Train Epoch: 101 [0/246 (0%)]\tLoss: 2.074745\n",
      "Train Epoch: 101 [100/246 (41%)]\tLoss: 0.064583\n",
      "Train Epoch: 101 [200/246 (81%)]\tLoss: 0.064417\n",
      "Train Epoch: 102 [0/246 (0%)]\tLoss: 2.074763\n",
      "Train Epoch: 102 [100/246 (41%)]\tLoss: 0.064576\n",
      "Train Epoch: 102 [200/246 (81%)]\tLoss: 0.064417\n",
      "Train Epoch: 103 [0/246 (0%)]\tLoss: 2.074782\n",
      "Train Epoch: 103 [100/246 (41%)]\tLoss: 0.064568\n",
      "Train Epoch: 103 [200/246 (81%)]\tLoss: 0.064416\n",
      "Train Epoch: 104 [0/246 (0%)]\tLoss: 2.074799\n",
      "Train Epoch: 104 [100/246 (41%)]\tLoss: 0.064561\n",
      "Train Epoch: 104 [200/246 (81%)]\tLoss: 0.064416\n",
      "Train Epoch: 105 [0/246 (0%)]\tLoss: 2.074817\n",
      "Train Epoch: 105 [100/246 (41%)]\tLoss: 0.064553\n",
      "Train Epoch: 105 [200/246 (81%)]\tLoss: 0.064415\n",
      "Train Epoch: 106 [0/246 (0%)]\tLoss: 2.074837\n",
      "Train Epoch: 106 [100/246 (41%)]\tLoss: 0.064546\n",
      "Train Epoch: 106 [200/246 (81%)]\tLoss: 0.064414\n",
      "Train Epoch: 107 [0/246 (0%)]\tLoss: 2.074855\n",
      "Train Epoch: 107 [100/246 (41%)]\tLoss: 0.064539\n",
      "Train Epoch: 107 [200/246 (81%)]\tLoss: 0.064413\n",
      "Train Epoch: 108 [0/246 (0%)]\tLoss: 2.074873\n",
      "Train Epoch: 108 [100/246 (41%)]\tLoss: 0.064531\n",
      "Train Epoch: 108 [200/246 (81%)]\tLoss: 0.064412\n",
      "Train Epoch: 109 [0/246 (0%)]\tLoss: 2.074892\n",
      "Train Epoch: 109 [100/246 (41%)]\tLoss: 0.064524\n",
      "Train Epoch: 109 [200/246 (81%)]\tLoss: 0.064411\n",
      "Train Epoch: 110 [0/246 (0%)]\tLoss: 2.074911\n",
      "Train Epoch: 110 [100/246 (41%)]\tLoss: 0.064516\n",
      "Train Epoch: 110 [200/246 (81%)]\tLoss: 0.064410\n",
      "Train Epoch: 111 [0/246 (0%)]\tLoss: 2.074930\n",
      "Train Epoch: 111 [100/246 (41%)]\tLoss: 0.064509\n",
      "Train Epoch: 111 [200/246 (81%)]\tLoss: 0.064409\n",
      "Train Epoch: 112 [0/246 (0%)]\tLoss: 2.074949\n",
      "Train Epoch: 112 [100/246 (41%)]\tLoss: 0.064501\n",
      "Train Epoch: 112 [200/246 (81%)]\tLoss: 0.064408\n",
      "Train Epoch: 113 [0/246 (0%)]\tLoss: 2.074968\n",
      "Train Epoch: 113 [100/246 (41%)]\tLoss: 0.064494\n",
      "Train Epoch: 113 [200/246 (81%)]\tLoss: 0.064406\n",
      "Train Epoch: 114 [0/246 (0%)]\tLoss: 2.074985\n",
      "Train Epoch: 114 [100/246 (41%)]\tLoss: 0.064487\n",
      "Train Epoch: 114 [200/246 (81%)]\tLoss: 0.064405\n",
      "Train Epoch: 115 [0/246 (0%)]\tLoss: 2.075006\n",
      "Train Epoch: 115 [100/246 (41%)]\tLoss: 0.064479\n",
      "Train Epoch: 115 [200/246 (81%)]\tLoss: 0.064403\n",
      "Train Epoch: 116 [0/246 (0%)]\tLoss: 2.075026\n",
      "Train Epoch: 116 [100/246 (41%)]\tLoss: 0.064472\n",
      "Train Epoch: 116 [200/246 (81%)]\tLoss: 0.064402\n",
      "Train Epoch: 117 [0/246 (0%)]\tLoss: 2.075045\n",
      "Train Epoch: 117 [100/246 (41%)]\tLoss: 0.064465\n",
      "Train Epoch: 117 [200/246 (81%)]\tLoss: 0.064400\n",
      "Train Epoch: 118 [0/246 (0%)]\tLoss: 2.075064\n",
      "Train Epoch: 118 [100/246 (41%)]\tLoss: 0.064457\n",
      "Train Epoch: 118 [200/246 (81%)]\tLoss: 0.064399\n",
      "Train Epoch: 119 [0/246 (0%)]\tLoss: 2.075085\n",
      "Train Epoch: 119 [100/246 (41%)]\tLoss: 0.064450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 119 [200/246 (81%)]\tLoss: 0.064397\n",
      "Train Epoch: 120 [0/246 (0%)]\tLoss: 2.075104\n",
      "Train Epoch: 120 [100/246 (41%)]\tLoss: 0.064443\n",
      "Train Epoch: 120 [200/246 (81%)]\tLoss: 0.064395\n",
      "Train Epoch: 121 [0/246 (0%)]\tLoss: 2.075123\n",
      "Train Epoch: 121 [100/246 (41%)]\tLoss: 0.064436\n",
      "Train Epoch: 121 [200/246 (81%)]\tLoss: 0.064394\n",
      "Train Epoch: 122 [0/246 (0%)]\tLoss: 2.075143\n",
      "Train Epoch: 122 [100/246 (41%)]\tLoss: 0.064429\n",
      "Train Epoch: 122 [200/246 (81%)]\tLoss: 0.064392\n",
      "Train Epoch: 123 [0/246 (0%)]\tLoss: 2.075162\n",
      "Train Epoch: 123 [100/246 (41%)]\tLoss: 0.064422\n",
      "Train Epoch: 123 [200/246 (81%)]\tLoss: 0.064390\n",
      "Train Epoch: 124 [0/246 (0%)]\tLoss: 2.075184\n",
      "Train Epoch: 124 [100/246 (41%)]\tLoss: 0.064415\n",
      "Train Epoch: 124 [200/246 (81%)]\tLoss: 0.064388\n",
      "Train Epoch: 125 [0/246 (0%)]\tLoss: 2.075203\n",
      "Train Epoch: 125 [100/246 (41%)]\tLoss: 0.064408\n",
      "Train Epoch: 125 [200/246 (81%)]\tLoss: 0.064386\n",
      "Train Epoch: 126 [0/246 (0%)]\tLoss: 2.075223\n",
      "Train Epoch: 126 [100/246 (41%)]\tLoss: 0.064401\n",
      "Train Epoch: 126 [200/246 (81%)]\tLoss: 0.064384\n",
      "Train Epoch: 127 [0/246 (0%)]\tLoss: 2.075242\n",
      "Train Epoch: 127 [100/246 (41%)]\tLoss: 0.064394\n",
      "Train Epoch: 127 [200/246 (81%)]\tLoss: 0.064382\n",
      "Train Epoch: 128 [0/246 (0%)]\tLoss: 2.075263\n",
      "Train Epoch: 128 [100/246 (41%)]\tLoss: 0.064388\n",
      "Train Epoch: 128 [200/246 (81%)]\tLoss: 0.064380\n",
      "Train Epoch: 129 [0/246 (0%)]\tLoss: 2.075284\n",
      "Train Epoch: 129 [100/246 (41%)]\tLoss: 0.064381\n",
      "Train Epoch: 129 [200/246 (81%)]\tLoss: 0.064378\n",
      "Train Epoch: 130 [0/246 (0%)]\tLoss: 2.075303\n",
      "Train Epoch: 130 [100/246 (41%)]\tLoss: 0.064374\n",
      "Train Epoch: 130 [200/246 (81%)]\tLoss: 0.064376\n",
      "Train Epoch: 131 [0/246 (0%)]\tLoss: 2.075323\n",
      "Train Epoch: 131 [100/246 (41%)]\tLoss: 0.064368\n",
      "Train Epoch: 131 [200/246 (81%)]\tLoss: 0.064373\n",
      "Train Epoch: 132 [0/246 (0%)]\tLoss: 2.075344\n",
      "Train Epoch: 132 [100/246 (41%)]\tLoss: 0.064361\n",
      "Train Epoch: 132 [200/246 (81%)]\tLoss: 0.064371\n",
      "Train Epoch: 133 [0/246 (0%)]\tLoss: 2.075363\n",
      "Train Epoch: 133 [100/246 (41%)]\tLoss: 0.065327\n",
      "Train Epoch: 133 [200/246 (81%)]\tLoss: 0.064388\n",
      "Train Epoch: 134 [0/246 (0%)]\tLoss: 2.073107\n",
      "Train Epoch: 134 [100/246 (41%)]\tLoss: 0.065318\n",
      "Train Epoch: 134 [200/246 (81%)]\tLoss: 0.064385\n",
      "Train Epoch: 135 [0/246 (0%)]\tLoss: 2.073208\n",
      "Train Epoch: 135 [100/246 (41%)]\tLoss: 0.065315\n",
      "Train Epoch: 135 [200/246 (81%)]\tLoss: 0.064380\n",
      "Train Epoch: 136 [0/246 (0%)]\tLoss: 2.073283\n",
      "Train Epoch: 136 [100/246 (41%)]\tLoss: 0.065311\n",
      "Train Epoch: 136 [200/246 (81%)]\tLoss: 0.064374\n",
      "Train Epoch: 137 [0/246 (0%)]\tLoss: 2.073342\n",
      "Train Epoch: 137 [100/246 (41%)]\tLoss: 0.065307\n",
      "Train Epoch: 137 [200/246 (81%)]\tLoss: 0.064367\n",
      "Train Epoch: 138 [0/246 (0%)]\tLoss: 2.073389\n",
      "Train Epoch: 138 [100/246 (41%)]\tLoss: 0.065303\n",
      "Train Epoch: 138 [200/246 (81%)]\tLoss: 0.064359\n",
      "Train Epoch: 139 [0/246 (0%)]\tLoss: 2.073426\n",
      "Train Epoch: 139 [100/246 (41%)]\tLoss: 0.065299\n",
      "Train Epoch: 139 [200/246 (81%)]\tLoss: 0.064351\n",
      "Train Epoch: 140 [0/246 (0%)]\tLoss: 2.073458\n",
      "Train Epoch: 140 [100/246 (41%)]\tLoss: 0.065295\n",
      "Train Epoch: 140 [200/246 (81%)]\tLoss: 0.064343\n",
      "Train Epoch: 141 [0/246 (0%)]\tLoss: 2.073486\n",
      "Train Epoch: 141 [100/246 (41%)]\tLoss: 0.065291\n",
      "Train Epoch: 141 [200/246 (81%)]\tLoss: 0.064336\n",
      "Train Epoch: 142 [0/246 (0%)]\tLoss: 2.073511\n",
      "Train Epoch: 142 [100/246 (41%)]\tLoss: 0.065288\n",
      "Train Epoch: 142 [200/246 (81%)]\tLoss: 0.064328\n",
      "Train Epoch: 143 [0/246 (0%)]\tLoss: 2.073532\n",
      "Train Epoch: 143 [100/246 (41%)]\tLoss: 0.065284\n",
      "Train Epoch: 143 [200/246 (81%)]\tLoss: 0.064321\n",
      "Train Epoch: 144 [0/246 (0%)]\tLoss: 2.073551\n",
      "Train Epoch: 144 [100/246 (41%)]\tLoss: 0.065281\n",
      "Train Epoch: 144 [200/246 (81%)]\tLoss: 0.064313\n",
      "Train Epoch: 145 [0/246 (0%)]\tLoss: 2.073568\n",
      "Train Epoch: 145 [100/246 (41%)]\tLoss: 0.065278\n",
      "Train Epoch: 145 [200/246 (81%)]\tLoss: 0.064306\n",
      "Train Epoch: 146 [0/246 (0%)]\tLoss: 2.073584\n",
      "Train Epoch: 146 [100/246 (41%)]\tLoss: 0.065275\n",
      "Train Epoch: 146 [200/246 (81%)]\tLoss: 0.064300\n",
      "Train Epoch: 147 [0/246 (0%)]\tLoss: 2.073595\n",
      "Train Epoch: 147 [100/246 (41%)]\tLoss: 0.065272\n",
      "Train Epoch: 147 [200/246 (81%)]\tLoss: 0.064293\n",
      "Train Epoch: 148 [0/246 (0%)]\tLoss: 2.073610\n",
      "Train Epoch: 148 [100/246 (41%)]\tLoss: 0.065269\n",
      "Train Epoch: 148 [200/246 (81%)]\tLoss: 0.064287\n",
      "Train Epoch: 149 [0/246 (0%)]\tLoss: 2.073623\n",
      "Train Epoch: 149 [100/246 (41%)]\tLoss: 0.065266\n",
      "Train Epoch: 149 [200/246 (81%)]\tLoss: 0.064281\n",
      "Train Epoch: 150 [0/246 (0%)]\tLoss: 2.073634\n",
      "Train Epoch: 150 [100/246 (41%)]\tLoss: 0.065264\n",
      "Train Epoch: 150 [200/246 (81%)]\tLoss: 0.064276\n",
      "Train Epoch: 151 [0/246 (0%)]\tLoss: 2.073645\n",
      "Train Epoch: 151 [100/246 (41%)]\tLoss: 0.065262\n",
      "Train Epoch: 151 [200/246 (81%)]\tLoss: 0.064270\n",
      "Train Epoch: 152 [0/246 (0%)]\tLoss: 2.073656\n",
      "Train Epoch: 152 [100/246 (41%)]\tLoss: 0.065259\n",
      "Train Epoch: 152 [200/246 (81%)]\tLoss: 0.064265\n",
      "Train Epoch: 153 [0/246 (0%)]\tLoss: 2.073663\n",
      "Train Epoch: 153 [100/246 (41%)]\tLoss: 0.065257\n",
      "Train Epoch: 153 [200/246 (81%)]\tLoss: 0.064261\n",
      "Train Epoch: 154 [0/246 (0%)]\tLoss: 2.073676\n",
      "Train Epoch: 154 [100/246 (41%)]\tLoss: 0.065255\n",
      "Train Epoch: 154 [200/246 (81%)]\tLoss: 0.064256\n",
      "Train Epoch: 155 [0/246 (0%)]\tLoss: 2.073684\n",
      "Train Epoch: 155 [100/246 (41%)]\tLoss: 0.065253\n",
      "Train Epoch: 155 [200/246 (81%)]\tLoss: 0.064251\n",
      "Train Epoch: 156 [0/246 (0%)]\tLoss: 2.073692\n",
      "Train Epoch: 156 [100/246 (41%)]\tLoss: 0.065251\n",
      "Train Epoch: 156 [200/246 (81%)]\tLoss: 0.064247\n",
      "Train Epoch: 157 [0/246 (0%)]\tLoss: 2.073701\n",
      "Train Epoch: 157 [100/246 (41%)]\tLoss: 0.065249\n",
      "Train Epoch: 157 [200/246 (81%)]\tLoss: 0.064243\n",
      "Train Epoch: 158 [0/246 (0%)]\tLoss: 2.073708\n",
      "Train Epoch: 158 [100/246 (41%)]\tLoss: 0.065247\n",
      "Train Epoch: 158 [200/246 (81%)]\tLoss: 0.064239\n",
      "Train Epoch: 159 [0/246 (0%)]\tLoss: 2.073715\n",
      "Train Epoch: 159 [100/246 (41%)]\tLoss: 0.065246\n",
      "Train Epoch: 159 [200/246 (81%)]\tLoss: 0.064235\n",
      "Train Epoch: 160 [0/246 (0%)]\tLoss: 2.073722\n",
      "Train Epoch: 160 [100/246 (41%)]\tLoss: 0.065244\n",
      "Train Epoch: 160 [200/246 (81%)]\tLoss: 0.064231\n",
      "Train Epoch: 161 [0/246 (0%)]\tLoss: 2.073728\n",
      "Train Epoch: 161 [100/246 (41%)]\tLoss: 0.065243\n",
      "Train Epoch: 161 [200/246 (81%)]\tLoss: 0.064228\n",
      "Train Epoch: 162 [0/246 (0%)]\tLoss: 2.073737\n",
      "Train Epoch: 162 [100/246 (41%)]\tLoss: 0.065241\n",
      "Train Epoch: 162 [200/246 (81%)]\tLoss: 0.064225\n",
      "Train Epoch: 163 [0/246 (0%)]\tLoss: 2.073744\n",
      "Train Epoch: 163 [100/246 (41%)]\tLoss: 0.065240\n",
      "Train Epoch: 163 [200/246 (81%)]\tLoss: 0.064221\n",
      "Train Epoch: 164 [0/246 (0%)]\tLoss: 2.073751\n",
      "Train Epoch: 164 [100/246 (41%)]\tLoss: 0.065238\n",
      "Train Epoch: 164 [200/246 (81%)]\tLoss: 0.064218\n",
      "Train Epoch: 165 [0/246 (0%)]\tLoss: 2.073758\n",
      "Train Epoch: 165 [100/246 (41%)]\tLoss: 0.065237\n",
      "Train Epoch: 165 [200/246 (81%)]\tLoss: 0.064215\n",
      "Train Epoch: 166 [0/246 (0%)]\tLoss: 2.073762\n",
      "Train Epoch: 166 [100/246 (41%)]\tLoss: 0.065236\n",
      "Train Epoch: 166 [200/246 (81%)]\tLoss: 0.064212\n",
      "Train Epoch: 167 [0/246 (0%)]\tLoss: 2.073768\n",
      "Train Epoch: 167 [100/246 (41%)]\tLoss: 0.065234\n",
      "Train Epoch: 167 [200/246 (81%)]\tLoss: 0.064209\n",
      "Train Epoch: 168 [0/246 (0%)]\tLoss: 2.073775\n",
      "Train Epoch: 168 [100/246 (41%)]\tLoss: 0.065233\n",
      "Train Epoch: 168 [200/246 (81%)]\tLoss: 0.064207\n",
      "Train Epoch: 169 [0/246 (0%)]\tLoss: 2.073781\n",
      "Train Epoch: 169 [100/246 (41%)]\tLoss: 0.065232\n",
      "Train Epoch: 169 [200/246 (81%)]\tLoss: 0.064204\n",
      "Train Epoch: 170 [0/246 (0%)]\tLoss: 2.073786\n",
      "Train Epoch: 170 [100/246 (41%)]\tLoss: 0.065231\n",
      "Train Epoch: 170 [200/246 (81%)]\tLoss: 0.064201\n",
      "Train Epoch: 171 [0/246 (0%)]\tLoss: 2.073792\n",
      "Train Epoch: 171 [100/246 (41%)]\tLoss: 0.065230\n",
      "Train Epoch: 171 [200/246 (81%)]\tLoss: 0.064199\n",
      "Train Epoch: 172 [0/246 (0%)]\tLoss: 2.073798\n",
      "Train Epoch: 172 [100/246 (41%)]\tLoss: 0.065229\n",
      "Train Epoch: 172 [200/246 (81%)]\tLoss: 0.064196\n",
      "Train Epoch: 173 [0/246 (0%)]\tLoss: 2.073802\n",
      "Train Epoch: 173 [100/246 (41%)]\tLoss: 0.065228\n",
      "Train Epoch: 173 [200/246 (81%)]\tLoss: 0.064194\n",
      "Train Epoch: 174 [0/246 (0%)]\tLoss: 2.073810\n",
      "Train Epoch: 174 [100/246 (41%)]\tLoss: 0.065227\n",
      "Train Epoch: 174 [200/246 (81%)]\tLoss: 0.064192\n",
      "Train Epoch: 175 [0/246 (0%)]\tLoss: 2.073813\n",
      "Train Epoch: 175 [100/246 (41%)]\tLoss: 0.065226\n",
      "Train Epoch: 175 [200/246 (81%)]\tLoss: 0.064190\n",
      "Train Epoch: 176 [0/246 (0%)]\tLoss: 2.073818\n",
      "Train Epoch: 176 [100/246 (41%)]\tLoss: 0.065225\n",
      "Train Epoch: 176 [200/246 (81%)]\tLoss: 0.064187\n",
      "Train Epoch: 177 [0/246 (0%)]\tLoss: 2.073824\n",
      "Train Epoch: 177 [100/246 (41%)]\tLoss: 0.065224\n",
      "Train Epoch: 177 [200/246 (81%)]\tLoss: 0.064185\n",
      "Train Epoch: 178 [0/246 (0%)]\tLoss: 2.073829\n",
      "Train Epoch: 178 [100/246 (41%)]\tLoss: 0.065223\n",
      "Train Epoch: 178 [200/246 (81%)]\tLoss: 0.064183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 179 [0/246 (0%)]\tLoss: 2.073833\n",
      "Train Epoch: 179 [100/246 (41%)]\tLoss: 0.065222\n",
      "Train Epoch: 179 [200/246 (81%)]\tLoss: 0.064182\n",
      "Train Epoch: 180 [0/246 (0%)]\tLoss: 2.073838\n",
      "Train Epoch: 180 [100/246 (41%)]\tLoss: 0.065221\n",
      "Train Epoch: 180 [200/246 (81%)]\tLoss: 0.064180\n",
      "Train Epoch: 181 [0/246 (0%)]\tLoss: 2.073841\n",
      "Train Epoch: 181 [100/246 (41%)]\tLoss: 0.065220\n",
      "Train Epoch: 181 [200/246 (81%)]\tLoss: 0.064178\n",
      "Train Epoch: 182 [0/246 (0%)]\tLoss: 2.073846\n",
      "Train Epoch: 182 [100/246 (41%)]\tLoss: 0.065220\n",
      "Train Epoch: 182 [200/246 (81%)]\tLoss: 0.064176\n",
      "Train Epoch: 183 [0/246 (0%)]\tLoss: 2.073851\n",
      "Train Epoch: 183 [100/246 (41%)]\tLoss: 0.065219\n",
      "Train Epoch: 183 [200/246 (81%)]\tLoss: 0.064174\n",
      "Train Epoch: 184 [0/246 (0%)]\tLoss: 2.073854\n",
      "Train Epoch: 184 [100/246 (41%)]\tLoss: 0.065218\n",
      "Train Epoch: 184 [200/246 (81%)]\tLoss: 0.064173\n",
      "Train Epoch: 185 [0/246 (0%)]\tLoss: 2.073861\n",
      "Train Epoch: 185 [100/246 (41%)]\tLoss: 0.065217\n",
      "Train Epoch: 185 [200/246 (81%)]\tLoss: 0.064171\n",
      "Train Epoch: 186 [0/246 (0%)]\tLoss: 2.073864\n",
      "Train Epoch: 186 [100/246 (41%)]\tLoss: 0.065217\n",
      "Train Epoch: 186 [200/246 (81%)]\tLoss: 0.064169\n",
      "Train Epoch: 187 [0/246 (0%)]\tLoss: 2.073868\n",
      "Train Epoch: 187 [100/246 (41%)]\tLoss: 0.065216\n",
      "Train Epoch: 187 [200/246 (81%)]\tLoss: 0.064168\n",
      "Train Epoch: 188 [0/246 (0%)]\tLoss: 2.073872\n",
      "Train Epoch: 188 [100/246 (41%)]\tLoss: 0.065215\n",
      "Train Epoch: 188 [200/246 (81%)]\tLoss: 0.064166\n",
      "Train Epoch: 189 [0/246 (0%)]\tLoss: 2.073875\n",
      "Train Epoch: 189 [100/246 (41%)]\tLoss: 0.065215\n",
      "Train Epoch: 189 [200/246 (81%)]\tLoss: 0.064165\n",
      "Train Epoch: 190 [0/246 (0%)]\tLoss: 2.073880\n",
      "Train Epoch: 190 [100/246 (41%)]\tLoss: 0.065214\n",
      "Train Epoch: 190 [200/246 (81%)]\tLoss: 0.064163\n",
      "Train Epoch: 191 [0/246 (0%)]\tLoss: 2.073882\n",
      "Train Epoch: 191 [100/246 (41%)]\tLoss: 0.065213\n",
      "Train Epoch: 191 [200/246 (81%)]\tLoss: 0.064162\n",
      "Train Epoch: 192 [0/246 (0%)]\tLoss: 2.073889\n",
      "Train Epoch: 192 [100/246 (41%)]\tLoss: 0.065213\n",
      "Train Epoch: 192 [200/246 (81%)]\tLoss: 0.064161\n",
      "Train Epoch: 193 [0/246 (0%)]\tLoss: 2.073891\n",
      "Train Epoch: 193 [100/246 (41%)]\tLoss: 0.065212\n",
      "Train Epoch: 193 [200/246 (81%)]\tLoss: 0.064159\n",
      "Train Epoch: 194 [0/246 (0%)]\tLoss: 2.073895\n",
      "Train Epoch: 194 [100/246 (41%)]\tLoss: 0.065212\n",
      "Train Epoch: 194 [200/246 (81%)]\tLoss: 0.064158\n",
      "Train Epoch: 195 [0/246 (0%)]\tLoss: 2.073898\n",
      "Train Epoch: 195 [100/246 (41%)]\tLoss: 0.065211\n",
      "Train Epoch: 195 [200/246 (81%)]\tLoss: 0.064157\n",
      "Train Epoch: 196 [0/246 (0%)]\tLoss: 2.073900\n",
      "Train Epoch: 196 [100/246 (41%)]\tLoss: 0.065210\n",
      "Train Epoch: 196 [200/246 (81%)]\tLoss: 0.064155\n",
      "Train Epoch: 197 [0/246 (0%)]\tLoss: 2.073907\n",
      "Train Epoch: 197 [100/246 (41%)]\tLoss: 0.065210\n",
      "Train Epoch: 197 [200/246 (81%)]\tLoss: 0.064154\n",
      "Train Epoch: 198 [0/246 (0%)]\tLoss: 2.073910\n",
      "Train Epoch: 198 [100/246 (41%)]\tLoss: 0.065209\n",
      "Train Epoch: 198 [200/246 (81%)]\tLoss: 0.064153\n",
      "Train Epoch: 199 [0/246 (0%)]\tLoss: 2.073912\n",
      "Train Epoch: 199 [100/246 (41%)]\tLoss: 0.065209\n",
      "Train Epoch: 199 [200/246 (81%)]\tLoss: 0.064152\n",
      "Train Loss: 130.925 | Accuracy: 10.163\n",
      "MSE: 0.473, RMSE: 0.687\n",
      "0.68745714\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "path = './data/qoe/pursue/simulated_qoe.csv'\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "\n",
    "\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "# define the network\n",
    "model = MLP(12)\n",
    "#model = NeuralNetwork()\n",
    "# train the model\n",
    "train_model(train_dl, model)\n",
    "# evaluate the model\n",
    "mse = evaluate_model(test_dl, model)\n",
    "rmse = sqrt(mse)\n",
    "print('MSE: %.3f, RMSE: %.3f' % (mse, sqrt(mse)))\n",
    "#make a single prediction (expect class=1)\n",
    "print(rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff89787e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[6.023859  0.7723446]]\n"
     ]
    }
   ],
   "source": [
    "row1 = [1,1,1,1,172423,4300,30,0,6,10,900,0.9] #0,6\n",
    "row2 = [1,1,1,1,538,1850,12,0,6,10,120,0.12]#,5,1\n",
    "row3 = [1,1,1,1,53463,750,4,11,6,10,20,0.02]#,3,1\n",
    "\n",
    "yhat = predict(row3, model)\n",
    "print('Predicted:',yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a435dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.save(model.state_dict(), './model/model_central_training.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c058fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmodel(path):\n",
    "    model = MLP(12)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3af59e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model/model_central_training.pt'\n",
    "model_loaded = loadmodel(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4f1fcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden1): Linear(in_features=12, out_features=12, bias=True)\n",
      "  (act1): Sigmoid()\n",
      "  (hidden2): Linear(in_features=12, out_features=6, bias=True)\n",
      "  (act2): Sigmoid()\n",
      "  (hidden3): Linear(in_features=6, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a09d414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[6.023859  0.7723446]]\n"
     ]
    }
   ],
   "source": [
    "yhat = predict(row2, model_loaded)\n",
    "print('Predicted:',yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0f73e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7723446"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16b4bbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.023859"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb9d7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
